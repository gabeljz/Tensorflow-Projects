{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input-Output\n",
    "import json\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "# Data manipulation\n",
    "# WARNING, sympy has a re() function which will conflicts with the regex library - \"re\"\n",
    "# it is safer to namespace the whole sympy library as \"sp\"\n",
    "# import sympy as sp \n",
    "import re\n",
    "import random\n",
    "\n",
    "# Programmed on\n",
    "# Keras version 2.0.8\n",
    "# Tensorflow version 1.3.0\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTION_MODE = \"construct_new_model\" # to contruct a new model\n",
    "EXECUTION_MODE = \"load_pretrained_model\" # to load a saved model that was trained previously\n",
    "\n",
    "NUM_OF_LEARNING_OBJECTIVES = \"all_in_binary_categories\"\n",
    "# NUM_OF_LEARNING_OBJECTIVES = \"all_used_in_binary_categories\"\n",
    "# NUM_OF_LEARNING_OBJECTIVES = 1\n",
    "\n",
    "LEARNING_OBJECTIVES_COUNTER = np.zeros(381, dtype=np.int_).tolist()\n",
    "\n",
    "if NUM_OF_LEARNING_OBJECTIVES == \"all_in_binary_categories\":\n",
    "    FILE_SUFFIX = \"in_binary_categories\"\n",
    "elif NUM_OF_LEARNING_OBJECTIVES == \"all_used_in_binary_categories\":\n",
    "    FILE_SUFFIX = \"all_used_in_binary_categories\"\n",
    "else:\n",
    "    FILE_SUFFIX = \"{}xLO_as_regression\".format(NUM_OF_LEARNING_OBJECTIVES)\n",
    "    \n",
    "MODEL_FILEPATH = \"Learning_objectives_classifier-LSTM_bidirectional_concat-{}.h5\".format(FILE_SUFFIX)\n",
    "\n",
    "# Global variables that are meant to be updated by the functions\n",
    "# Before removing brackets was 356\n",
    "# After removing brackets is 221\n",
    "MAX_LENGTH = 0;\n",
    "\n",
    "# Constants for training\n",
    "MAX_LENGTH_FOR_TRAINING = 221\n",
    "ENCODING_SIZE_FOR_TRAINING = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click to jump to [\"5. Load encoded data and train Model\"](http://localhost:8888/notebooks/projects/Workings_to_Meta/Identify_learning_objectives_from_workings.ipynb#5.-Load-encoded-data-and-train-Model)\n",
    "\n",
    "# 1. Identify Learning Objectives from Student's Working\n",
    "\n",
    "## 1.1. Goal of this project\n",
    "\n",
    "This project is a proof-of-concept for possible advancement in the capabilites of the product that I am working on.\n",
    "\n",
    "The product is a online platform which can let students attempt Maths questions with full workings, and the system will automatically mark every step as correct or wrong.\n",
    "\n",
    "Currently, the system can mark a working to a question as either fully correct or highlight the mistakes to the students.\n",
    "\n",
    "However, if a working has been marked as fully correct, the system can not detect if the student had achieved the learnings objectives that had been set in the question.\n",
    "\n",
    "Every question has been authored with a **set of learning objectives** and the author had provided **a full working that achieved all the learning objectives**.\n",
    "\n",
    "The objective of this project is to indentify the learning objectives that has been achieved in a working, hence when the student enter his working, the model can be used to predict the learning objectives achieved by him, and match against the learning objectives that was set for the question by the author.\n",
    "\n",
    "\n",
    "\n",
    "# 1.2. Conclusion\n",
    "\n",
    "Using a BiDirectional LSTM model to train on the current set of data results in one of two problems:\n",
    "\n",
    "1. Overfitting when trained to output binary categories\n",
    "2. Apparently random predictions when trained to output as regression.\n",
    "\n",
    "This project proves that trying to predict the learning objectives achieved by the students based on his whole working is not feasible.\n",
    "\n",
    "\n",
    "# 1.3. Further Work\n",
    "\n",
    "1. Try a All-You-Need-Is-Attention model on the same input and output data.\n",
    "2. Try to encode the input data into a 2D picture and try to use a CNN model to train on it.\n",
    "3. Try to build a model to detect the types of equations in a working.\n",
    "    * While predicting the learning objectives achieved in a whole working might not be feasible,\n",
    "    * it might still be possible to predict the type of equations on the individual equations in the whole working.\n",
    "    * Then another model can use the predicted types of equations,\n",
    "    * to further predict the learning objectives achieved.\n",
    "    * However, presently, we do not have enough labelled equations to train on this approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Output encoding and model\n",
    "\n",
    "There are 380 possible learning objectives in the system. Their identifiers start from 1 to 380.\n",
    "\n",
    "Every question has a set of learning objectives, which can have one or more elements.\n",
    "\n",
    "The learning objectives in each questions have been arranged from highest to lowest priority.\n",
    "\n",
    "Three different encoding scheme had been explored, and each scheme will require a different output layer in the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_learning_objectives(LOs):\n",
    "    global LEARNING_OBJECTIVES_COUNTER\n",
    "    for LO in LOs:\n",
    "        LEARNING_OBJECTIVES_COUNTER[LO] += 1\n",
    "\n",
    "    if NUM_OF_LEARNING_OBJECTIVES == \"all_in_binary_categories\":\n",
    "        # binary for all\n",
    "        learning_objectives = np.zeros(381, dtype=np.int_)\n",
    "        learning_objectives[LOs] = 1\n",
    "        return learning_objectives.tolist()\n",
    "    elif NUM_OF_LEARNING_OBJECTIVES == \"all_used_in_binary_categories\":\n",
    "        learning_objectives = np.zeros(len(LEARNING_OBJECTIVES_IN_USED), dtype=np.int_)\n",
    "        for LO in LOs:\n",
    "            learning_objectives[LEARNING_OBJECTIVES_IN_USED.index(LO)] = 1\n",
    "        return learning_objectives.tolist()\n",
    "    else:\n",
    "        # top N number of LOs\n",
    "        N = NUM_OF_LEARNING_OBJECTIVES\n",
    "        LOs = LOs[0:N]\n",
    "        LOs.extend([0]*(N-len(LOs)))\n",
    "        LOs = [x/380 for x in LOs]\n",
    "        return LOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Binary Categories for all Learning Objectives\n",
    "\n",
    "The obvious choice for encoding the output is binary categories for all 380 learning objectives.\n",
    "\n",
    "The output layer of the Model is then configured as such:\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output = Dense(381, activation ='sigmoid')(x)\n",
    "bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "bidirectional_concat_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "After some training, it was apparent that there are a few problems\n",
    "\n",
    "1. The training time is too long.\n",
    "2. The output is always 21, suggesting that the Model is over-fitting.\n",
    "3. The accuracy has plateaued at 0.1065, and fluctuates a little.\n",
    "\n",
    "But most of the learning objectives are not used in the system, and hence are always set to 0.\n",
    "\n",
    "Hence the learning objectives was trimmed by the encoding scheme - \"Binary Categories for used Learning Objectives\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_LEARNING_OBJECTIVES_STORE = NUM_OF_LEARNING_OBJECTIVES\n",
    "NUM_OF_LEARNING_OBJECTIVES = \"all_in_binary_categories\"\n",
    "\n",
    "learning_objectives = [1, 3, 5, 7, 101, 216, 323]\n",
    "encoded = encode_learning_objectives(learning_objectives)\n",
    "print(np.array(encoded).shape)\n",
    "print(encoded)\n",
    "\n",
    "NUM_OF_LEARNING_OBJECTIVES = NUM_OF_LEARNING_OBJECTIVES_STORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Binary Categories for used Learning Objectives\n",
    "\n",
    "Learning objectives that are not used in the system are removed from the training output.\n",
    "\n",
    "This reduces the possible output from 380 to 132.\n",
    "\n",
    "The output layer of the Model is then configured as such:\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output = Dense(len(LEARNING_OBJECTIVES_IN_USED), activation ='sigmoid')(x)\n",
    "bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "bidirectional_concat_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "After some training, it was apparent that there are a few problems\n",
    "\n",
    "1. The output is always 21, suggesting that the Model is over-fitting.\n",
    "2. The accuracy has plateaued at around 0.1065, and fluctuates a little.\n",
    "\n",
    "To totally avoid over-fitting,  the output is forced to become a single value, hence the next encoding scheme is used - \"Linear Regression for the first N Learning Objectives\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_OBJECTIVES_IN_USED = [\n",
    "    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 19, 20,\n",
    "    21, 22, 23, 24, 25, 27, 28, 31, 35, 37, 38, 39, 40, 42, 44,\n",
    "    45, 55, 56, 57, 58, 59, 60, 76, 77, 78, 79, 82, 83, 84, 85,\n",
    "    86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100,\n",
    "    101, 102, 104, 105, 107, 108, 109, 112, 113, 114, 115, 116,\n",
    "    117, 118, 120, 121, 125, 126, 127, 132, 133, 134, 135, 136,\n",
    "    137, 138, 141, 142, 143, 144, 145, 158, 159, 160, 161, 163,\n",
    "    169, 182, 183, 184, 185, 216, 217, 218, 223, 225, 226, 262,\n",
    "    263, 280, 281, 282, 283, 285, 289, 290, 291, 292, 293, 294,\n",
    "    295, 296, 297, 298, 303, 305, 306, 307, 308, 323\n",
    "]\n",
    "\n",
    "NUM_OF_LEARNING_OBJECTIVES_STORE = NUM_OF_LEARNING_OBJECTIVES\n",
    "NUM_OF_LEARNING_OBJECTIVES = \"all_used_in_binary_categories\"\n",
    "\n",
    "learning_objectives = [1, 3, 5, 7, 101, 216, 323]\n",
    "encoded = encode_learning_objectives(learning_objectives)\n",
    "print(np.array(encoded).shape)\n",
    "print(encoded)\n",
    "\n",
    "NUM_OF_LEARNING_OBJECTIVES = NUM_OF_LEARNING_OBJECTIVES_STORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Linear Regression for the first N Learning Objectives\n",
    "\n",
    "Only the first Learning Objectives, which is of the highest priority for the question, is used as the output.\n",
    "\n",
    "The first learning objective is normalized by dividing by 380 to result in a float.\n",
    "\n",
    "The Model is trained and used to predict this floating value, which will be decoded by multiplying by 380.\n",
    "\n",
    "The output layer of the Model is then configured as such:\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output = Dense(NUM_OF_LEARNING_OBJECTIVES, activation ='linear')(x)\n",
    "bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "bidirectional_concat_model.compile(loss='mse', optimizer=adam, metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "After some training, it was apparent that there are a few problems\n",
    "\n",
    "1. The predicted output is random, and has no observable pattern with the actual target.\n",
    "    * While the learning objectives are indexed in such a way that they are similiar to their neighbours, this suggest that the similiarity of neighbouring learning objectives are not detectable by the Model.\n",
    "    * The training input do not contain sufficient informations. Some critical signals are missing from the input, and can not be encoded into a stream of values easily.\n",
    "2. The training accuracy has plateaued at 0.0187.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_LEARNING_OBJECTIVES_STORE = NUM_OF_LEARNING_OBJECTIVES\n",
    "NUM_OF_LEARNING_OBJECTIVES = 1\n",
    "\n",
    "learning_objectives = [1, 3, 5, 7, 101, 216, 323]\n",
    "encoded = encode_learning_objectives(learning_objectives)\n",
    "print(np.array(encoded).shape)\n",
    "print(encoded)\n",
    "\n",
    "NUM_OF_LEARNING_OBJECTIVES = NUM_OF_LEARNING_OBJECTIVES_STORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Input encoding\n",
    "\n",
    "One of the biggest challenges in this project initially, is encoding of the input, as the input data are workings that are made up of Mathematical symbols and operator.\n",
    "\n",
    "While the students will actually enter their workings in Latex, these Latex working had been converted to a format that is compatible with a Computer Algebra System for processing by the system.\n",
    "\n",
    "These converted workings will be used as the input to this project.\n",
    "\n",
    "## 3.1. One-hot encoding\n",
    "\n",
    "An obvious choice in encoding scheme is one hot encoding.\n",
    "\n",
    "The converted workings contain Mathematical operators and symbols.\n",
    "\n",
    "Among all the workings provided by the author, **44 classes of operators and symbols** have been identified.\n",
    "\n",
    "Each of these classes are been assigned with a one-hot encoding.\n",
    "\n",
    "Then the whole working is tokenised and converted to this one-hot encoding.\n",
    "\n",
    "With this encoding scheme, the longest sequnce has a length of 356 encoded tokens, and each encoded token has a size of 44."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS_ENCODING_LIST = [\n",
    "    'bracket_depth', 'newline',\n",
    "    '(', ')',\n",
    "    '=', '<=', '>=', '<', '>',\n",
    "    '+', '-', '*', '/', '^',\n",
    "    '&%', '&:', '&`*', '&p`', '&`',\n",
    "    'sin', 'cos', 'tan', 'sqrt', '%pi',\n",
    "    'qn_var', 'qn_var_para', 'declared_var', 'declared_var_para', 'ambiguous', \n",
    "    'dollar', 'cent', 'degree',\n",
    "    'hour', 'minute', 's',\n",
    "    'l', 'g', 'm',\n",
    "    'k', 'c', 'milli',\n",
    "    'whole', 'decimal', 'mix_frac',\n",
    "    'arbi_unit', 'arbi_unit_para',\n",
    "    'func',        \n",
    "]\n",
    "ENCODING_SIZE = len(TOKENS_ENCODING_LIST);\n",
    "ENCODING_COUNTER = np.zeros(ENCODING_SIZE, dtype=np.int_)\n",
    "    \n",
    "def encode_mx_token(token):\n",
    "    global ENCODING_COUNTER\n",
    "    encoded_mx_token = np.zeros(len(TOKENS_ENCODING_LIST), dtype=np.int_)\n",
    "    idx = TOKENS_ENCODING_LIST.index(token)\n",
    "    encoded_mx_token[idx] = 1\n",
    "    ENCODING_COUNTER[idx] += 1\n",
    "    return encoded_mx_token.tolist()\n",
    "\n",
    "def encode_by_fixed_str(enable, mx_str, needle, encoded_list):\n",
    "    if enable and mx_str[0:len(needle)] == needle:\n",
    "        mx_str = mx_str[len(needle):]\n",
    "        encoded_list.append(encode_mx_token(needle))\n",
    "        enable = False\n",
    "    return enable, mx_str, encoded_list\n",
    "\n",
    "def encode_by_regex(enable, mx_str, regex_seed, encoded_list, encoding):\n",
    "    regex = r'' + regex_seed\n",
    "    if enable and re.match(regex, mx_str):\n",
    "        m = re.match(regex, mx_str)\n",
    "        m = m.group(1)\n",
    "\n",
    "        encoded = encode_mx_token(encoding)\n",
    "\n",
    "        if encoding+\"_para\" in TOKENS_ENCODING_LIST:\n",
    "            para = re.search(r'\\d+\\.?\\d*', m)\n",
    "            if para is not None:\n",
    "                para = float(para.group(0))\n",
    "                encoded[TOKENS_ENCODING_LIST.index(encoding+\"_para\")] = para/10\n",
    "                ENCODING_COUNTER[TOKENS_ENCODING_LIST.index(encoding+\"_para\")] = \\\n",
    "                max(ENCODING_COUNTER[TOKENS_ENCODING_LIST.index(encoding+\"_para\")], para)\n",
    "\n",
    "        encoded_list.append(encoded)\n",
    "        mx_str = mx_str[len(m):]\n",
    "        enable = False\n",
    "    return enable, mx_str, encoded_list\n",
    "\n",
    "def encode_mx_string(mx_str):\n",
    "    encoded_list = []\n",
    "    remaining = \"\"\n",
    "\n",
    "    while(len(mx_str) > 0):\n",
    "        enable = True;\n",
    "\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"newline\", encoded_list)\n",
    "\n",
    "        # Customised Function with Brackets and commands\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(T_ud\\([^)]+\\))\", encoded_list, \"func\")\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(mix_frac\\(\\d+\\s*,\\s*\\d+\\s*\\/\\s*\\d+\\s*\\))\", encoded_list, \"mix_frac\")\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(select_symbol\\([^)]+\\))\", encoded_list, \"ambiguous\")\n",
    "\n",
    "        # Brackets\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"(\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \")\", encoded_list)\n",
    "\n",
    "        # Relational operator\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \">=\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"<=\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"=\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \">\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"<\", encoded_list)\n",
    "\n",
    "        # Arithmetic operator\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"+\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"-\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"*\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"/\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"^\", encoded_list)\n",
    "\n",
    "        # Custome Arithmetic operator\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"&%\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"&:\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"&p`\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"&`*\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"&`\", encoded_list)\n",
    "\n",
    "        # Standard functions\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"sin\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"cos\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"tan\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"sqrt\", encoded_list)\n",
    "\n",
    "        # Constant\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"%pi\", encoded_list)\n",
    "\n",
    "        # variable\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(d\\d+)\", encoded_list, \"declared_var\")\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(o\\d+)\", encoded_list, \"qn_var\")\n",
    "\n",
    "        # Arbitrary unit\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(u\\d+)\", encoded_list, \"arbi_unit\")\n",
    "\n",
    "        # numeric\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(\\d+\\.\\d+)\", encoded_list, \"decimal\")\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(\\d+)\", encoded_list, \"whole\")\n",
    "\n",
    "        # Units\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"dollar\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"cent\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"degree\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"hour\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"minute\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"s\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"l\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"g\", encoded_list)\n",
    "\n",
    "        # Prefixes\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"k\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"c\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(m)\\s*&`*\", encoded_list, \"milli\")\n",
    "\n",
    "        # Unit meter\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"m\", encoded_list)\n",
    "\n",
    "        if enable:\n",
    "            remaining += mx_str[0]\n",
    "            mx_str = mx_str[1:]        \n",
    "\n",
    "#     print(\"\\nencoded:\\n\", encoded_list)\n",
    "    remaining = remaining.replace(\" \", \"\")\n",
    "\n",
    "#     print(\"\\nremaining:\\n\", remaining)\n",
    "    if len(remaining) > 0:\n",
    "        raise ValueError(\"Not encoded:\", remaining)\n",
    "\n",
    "    return encoded_list\n",
    "\n",
    "test_mx_str = \"1+(2-3)*4\"\n",
    "test_encoded_list = encode_mx_string(test_mx_str)\n",
    "print(np.array(test_encoded_list).shape)\n",
    "print(np.array(test_encoded_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. One-hot encoding with 1X class parameter\n",
    "\n",
    "By representing all variables as a single class, too much information is removed in the final encoding.\n",
    "\n",
    "So for each of the variable class, a class parameter is added to the encoding.\n",
    "\n",
    "There are three types of variables used in the system, so three class parameters are added for the three variables classes.\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "TOKENS_ENCODING_LIST = [\n",
    "    ...\n",
    "    'qn_var',            # First type of variables - Variables defined by questions\n",
    "    'qn_var_para',       # Class parameter for the first type of variables\n",
    "    'declared_var',      # Second type of variables - Variables declared by students\n",
    "    'declared_var_para', # Class parameter for the second type of variables\n",
    "    'arbi_unit',         # Third type of variables - Variables which represent a unit\n",
    "    'arbi_unit_para',    # Class parameter for the third type of variables\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "The class parameter actually represent the index of the variable in the question, starting from **1**.\n",
    "\n",
    "For example, in a question where four variables are used - `[w, x, y, z]`.\n",
    "\n",
    "Then the indices of these four variables are -`[1, 2, 3, 4]`.\n",
    "\n",
    "They are then normalised by dividing by 10 and assigned to the class parameters - `[0.1, 0.2, 0.3, 0.4]`.\n",
    "\n",
    "Hence each variable is represented by two values in the encoding (\"qn_var\" is idx 24 and \"qn_var_para\" is 25, before removing brackets):\n",
    "\n",
    "* `encoded[24:26] = [1, 0.1]` is for \"w\"\n",
    "* `encoded[24:26] = [1, 0.2]` is for \"x\"\n",
    "* `encoded[24:26] = [1, 0.3]` is for \"y\"\n",
    "* `encoded[24:26] = [1, 0.4]` is for \"z\"\n",
    "\n",
    "With this encoding scheme, each the size of encoded token is increased from 44 to 47."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mx_str = \"o1+(o2-o3)*o4\" # converted from latex \"w+(x-y)*z\"\n",
    "test_encoded_list = encode_mx_string(test_mx_str)\n",
    "\n",
    "print(np.array(test_encoded_list).shape)\n",
    "\n",
    "all_qn_var_para = [encoded[TOKENS_ENCODING_LIST.index(\"qn_var_para\")] for encoded in test_encoded_list]\n",
    "print(\"All question variables parameters:\", all_qn_var_para)\n",
    "print('Encoding for \"w\"', test_encoded_list[0][24:26])\n",
    "print('Encoding for \"x\"', test_encoded_list[3][24:26])\n",
    "print('Encoding for \"y\"', test_encoded_list[5][24:26])\n",
    "print('Encoding for \"z\"', test_encoded_list[8][24:26])\n",
    "print(np.array(test_encoded_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. One-hot encoding with depth of bracket\n",
    "\n",
    "The vanilla one-hot encoding result in a rather long sequence.\n",
    "\n",
    "In Maths, the brackets are very often used, hence it is present in all workings and might not be a useful signal for our model.\n",
    "\n",
    "However, the contents within a pair of bracket has a different schematic meaning from the content outside the brackets.\n",
    "\n",
    "For every token, the depth of its surrounding brackets are counted and encoded as a decimal with a 0.1 increment per level.\n",
    "\n",
    "This encoded depth  is then included in the encoding.\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "TOKENS_ENCODING_LIST = [\n",
    "    ...\n",
    "    'bracket_depth',\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "Then the encoded bracket tokens are removed from the encoded sequence.\n",
    "\n",
    "With this encoding scheme, the longest sequnce is reduce from a length of 356 to a length of 221 encoded tokens.\n",
    "\n",
    "The size of each encoded token if reduced from 47 to 45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the content within the brackets should have the new depth\n",
    "# The brackets themselves are still at the same depth as outside the bracket\n",
    "def encode_bracket_depth(encoded_list):\n",
    "    global ENCODING_COUNTER\n",
    "    depth_idx = TOKENS_ENCODING_LIST.index(\"bracket_depth\")\n",
    "    lb_indx = TOKENS_ENCODING_LIST.index(\"(\")\n",
    "    rb_indx = TOKENS_ENCODING_LIST.index(\")\")\n",
    "\n",
    "    depth = 0\n",
    "\n",
    "    for encoded in encoded_list:\n",
    "        if encoded[lb_indx] == 1:\n",
    "            encoded[depth_idx] = depth/10\n",
    "            depth += 1\n",
    "        elif encoded[rb_indx] == 1:\n",
    "            depth -= 1\n",
    "            encoded[depth_idx] = depth/10\n",
    "        else:\n",
    "            encoded[depth_idx] = depth/10\n",
    "\n",
    "        ENCODING_COUNTER[depth_idx] = max(ENCODING_COUNTER[depth_idx], depth)\n",
    "\n",
    "    return encoded_list\n",
    "\n",
    "def change_tokens_to_None(encoded_list, token_indices):    \n",
    "    for token_idx in token_indices:\n",
    "        encoded_indices = [i for i, encoded in enumerate(encoded_list) if encoded[token_idx] == 1]\n",
    "        for idx in encoded_indices:\n",
    "            encoded_list[int(idx)] = [None] * len(encoded_list[idx])\n",
    "        encoded_indices = [i for i, encoded in enumerate(encoded_list) if encoded[token_idx] == 1]\n",
    "        for encoded in encoded_list:\n",
    "            encoded[token_idx] = None\n",
    "    return encoded_list\n",
    "\n",
    "def purge_None(encoded_list):\n",
    "    encoded_indices = [i for i, encoded in enumerate(encoded_list) if all(v is None for v in encoded)]\n",
    "    for idx in sorted(encoded_indices, reverse=True):\n",
    "        del encoded_list[idx]\n",
    "    encoded_list = [list(filter(lambda x: x is not None, encoded)) for encoded in encoded_list]\n",
    "    return encoded_list\n",
    "\n",
    "test_mx_str = \"1+(2-3)*4\"\n",
    "test_encoded_list = encode_mx_string(test_mx_str)\n",
    "print(\"Before encoding and removing bracket tokens:\")\n",
    "print(np.array(test_encoded_list).shape)\n",
    "print(np.array(test_encoded_list))\n",
    "\n",
    "# Encode bracket depth \n",
    "test_encoded_list = encode_bracket_depth(test_encoded_list)\n",
    "# Remove brackets\n",
    "lb_idx = TOKENS_ENCODING_LIST.index(\"(\")\n",
    "rb_idx = TOKENS_ENCODING_LIST.index(\")\")\n",
    "token_indices = [lb_idx, rb_idx]\n",
    "test_encoded_list = change_tokens_to_None(test_encoded_list, token_indices)\n",
    "test_encoded_list = purge_None(test_encoded_list)\n",
    "print(\"After encoding and removing bracket tokens:\")\n",
    "print(np.array(test_encoded_list).shape)\n",
    "print(np.array(test_encoded_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. One-hot encoding with depth of bracket and 1X class parameter, no less-used classes\n",
    "\n",
    "The \"sin\", \"cos\", and \"tan\" are only used 6 times each in the training samples.\n",
    "\n",
    "Hence they are removed from the encoding.\n",
    "\n",
    "With this encoding scheme, the longest sequence is still at a length of 221 encoded tokens, as it does not contain any of the \"sin\", \"cos\", and \"tan\" tokens.\n",
    "\n",
    "Each encoded token has been reduced from a size of 45 to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mx_str = \"sin(%pi)+cos(60&`(degree))+tan(o1)\" # converted from latex \"\\sin(\\pi)+\\cos(60\\degree)-\\tan(x)\"\n",
    "test_encoded_list = encode_mx_string(test_mx_str)\n",
    "\n",
    "# Encode bracket depth \n",
    "test_encoded_list = encode_bracket_depth(test_encoded_list)\n",
    "# Remove brackets\n",
    "lb_idx = TOKENS_ENCODING_LIST.index(\"(\")\n",
    "rb_idx = TOKENS_ENCODING_LIST.index(\")\")\n",
    "token_indices = [lb_idx, rb_idx]\n",
    "test_encoded_list = change_tokens_to_None(test_encoded_list, token_indices)\n",
    "\n",
    "# Remove trigo function\n",
    "sin_idx = TOKENS_ENCODING_LIST.index(\"sin\")\n",
    "cos_idx = TOKENS_ENCODING_LIST.index(\"cos\")\n",
    "tan_idx = TOKENS_ENCODING_LIST.index(\"tan\")\n",
    "token_indices = [sin_idx, cos_idx, tan_idx]\n",
    "test_encoded_list = change_tokens_to_None(test_encoded_list, token_indices)\n",
    "\n",
    "test_encoded_list = purge_None(test_encoded_list)\n",
    "\n",
    "print(np.array(test_encoded_list).shape)\n",
    "print(np.array(test_encoded_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Main encoding function: encode_and_save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_and_save_file(filepath):\n",
    "    global MAX_LENGTH\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    with open(filepath) as json_data:\n",
    "\n",
    "        raw_data = json.load(json_data)\n",
    "        \n",
    "        learning_objectives_raw = raw_data.get(\"learning_objectives\", [])\n",
    "        learning_objectives = encode_learning_objectives(learning_objectives_raw)       \n",
    "    \n",
    "        working_in_mx = raw_data.get(\"working_in_mx\", [])\n",
    "        encoded_mx_working = \"newline\".join(working_in_mx);\n",
    "        encoded_mx_working = encode_mx_string(encoded_mx_working)\n",
    "        \n",
    "        # Encode bracket's depth\n",
    "        encoded_mx_working = encode_bracket_depth(encoded_mx_working)\n",
    "        # Remove brackets\n",
    "        lb_idx = TOKENS_ENCODING_LIST.index(\"(\")\n",
    "        rb_idx = TOKENS_ENCODING_LIST.index(\")\")\n",
    "        token_indices = [lb_idx, rb_idx]\n",
    "        encoded_mx_working = change_tokens_to_None(encoded_mx_working, token_indices)\n",
    "\n",
    "        # Remove trigo function\n",
    "        sin_idx = TOKENS_ENCODING_LIST.index(\"sin\")\n",
    "        cos_idx = TOKENS_ENCODING_LIST.index(\"cos\")\n",
    "        tan_idx = TOKENS_ENCODING_LIST.index(\"tan\")\n",
    "        token_indices = [sin_idx, cos_idx, tan_idx]\n",
    "        encoded_mx_working = change_tokens_to_None(encoded_mx_working, token_indices)\n",
    "\n",
    "        encoded_mx_working = purge_None(encoded_mx_working)\n",
    "        MAX_LENGTH = max(MAX_LENGTH or 0, len(encoded_mx_working))\n",
    "        \n",
    "    m = re.search(r'qn_(\\d+)', filename);\n",
    "    if int(m.group(1)) % 5 == 0:\n",
    "        folder = \"./encoded/{}/test/\".format(FILE_SUFFIX)\n",
    "    else:\n",
    "        folder = \"./encoded/{}/train/\".format(FILE_SUFFIX)\n",
    "\n",
    "    filepath = \"{folder}{filename}\".format(folder=folder, filename=filename)\n",
    "    with open(filepath, \"w\") as outfile:\n",
    "        encoded_data = {\n",
    "            'learning_objectives' : learning_objectives,\n",
    "            'mx_working' : encoded_mx_working\n",
    "        }\n",
    "        json.dump(encoded_data, outfile)\n",
    "\n",
    "# print(encoded_mx_working)\n",
    "# print(learning_objectives)\n",
    "# pprint(working_in_mx)\n",
    "# pprint(working_in_texx)\n",
    "# pprint(std_work)\n",
    "# pprint(std_anss)\n",
    "\n",
    "# print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 0\n",
    "ENCODING_COUNTER = np.zeros(ENCODING_SIZE, dtype=np.int_)\n",
    "LEARNING_OBJECTIVES_COUNTER = np.zeros(381, dtype=np.int_).tolist()\n",
    "\n",
    "for filename in os.listdir(\"./raw/\"):\n",
    "    if(filename != \".DS_Store\"):\n",
    "        encode_and_save_file(\"./raw/\" + filename)\n",
    "\n",
    "print(\"Final Maximum length of sequence: \", MAX_LENGTH)\n",
    "\n",
    "print(pd.DataFrame({\n",
    "    'Token': TOKENS_ENCODING_LIST,\n",
    "    'Count': ENCODING_COUNTER\n",
    "}))\n",
    "\n",
    "print(pd.DataFrame({\n",
    "    'Learning Objectives': [i for i, v in enumerate(LEARNING_OBJECTIVES_COUNTER) if v >0],\n",
    "    'Count': [v for v in LEARNING_OBJECTIVES_COUNTER if v >0]\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load encoded data and train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_train(list_of_histories):\n",
    "    \n",
    "    if 'acc' in list_of_histories[0].history:\n",
    "        meas='acc'\n",
    "        loc='lower right'\n",
    "    else:\n",
    "        meas='loss'\n",
    "        loc='upper right'\n",
    "    \n",
    "    train_meas = []\n",
    "    val_meas = []\n",
    "    for hist in list_of_histories:\n",
    "        train_meas = train_meas + hist.history[meas]\n",
    "        val_meas = val_meas + hist.history['val_'+meas]\n",
    "\n",
    "    plt.plot(train_meas)\n",
    "    plt.plot(val_meas)\n",
    "    plt.title('model '+meas)\n",
    "    plt.ylabel(meas)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_encoded_data(filepath):\n",
    "    with open(filepath) as json_data:\n",
    "        raw_data = json.load(json_data)\n",
    "    return raw_data[\"mx_working\"] , raw_data[\"learning_objectives\"]\n",
    "\n",
    "def batch_generator(folder, batch_size):\n",
    "    batch_size = 1550 # overriding the passed in batch size\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "\n",
    "        if(filename != \".DS_Store\"):\n",
    "            x, y = load_encoded_data(folder + filename)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "\n",
    "    print(\"\\nFrom folder {folder}, loaded {num_samples} samples.\".format(num_samples=len(X), folder=folder))\n",
    "\n",
    "    for x in X:\n",
    "        x.extend([[0] * ENCODING_SIZE_FOR_TRAINING] * (MAX_LENGTH_FOR_TRAINING - len(x)))\n",
    "\n",
    "    # Capping batch size to the size of the array   \n",
    "    batch_size = min(len(X), batch_size)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    while True:\n",
    "        indices_random = random.sample(range(len(X)), batch_size)\n",
    "#         print(\"Num of samples: {} idx: \".format(len(indices_random)), indices_random[0:5])\n",
    "        yield X[indices_random], Y[indices_random]\n",
    "\n",
    "\n",
    "print(\"MAX_LENGTH_FOR_TRAINING:\", MAX_LENGTH_FOR_TRAINING)\n",
    "print(\"ENCODING_SIZE_FOR_TRAINING:\", ENCODING_SIZE_FOR_TRAINING)\n",
    "\n",
    "BATCH_SIZE = 1550\n",
    "\n",
    "training_batch_generator = batch_generator(\"./encoded/{}/train/\".format(FILE_SUFFIX), BATCH_SIZE)\n",
    "X_train, Y_train = next(training_batch_generator)\n",
    "print(\"Batched Input training shape: \", X_train.shape)\n",
    "print(\"Batched Output training shape: \", Y_train.shape)\n",
    "\n",
    "testing_batch_generator = batch_generator(\"./encoded/{}/test/\".format(FILE_SUFFIX), int(BATCH_SIZE/4))\n",
    "X_test, Y_test = next(testing_batch_generator)\n",
    "\n",
    "print(\"Batched Input testing shape: \", X_test.shape)\n",
    "print(\"Batched Output testing shape: \", Y_test.shape)\n",
    "\n",
    "print(X_test[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_of_histories = []\n",
    "\n",
    "if(EXECUTION_MODE == \"load_pretrained_model\" or EXECUTION_MODE == \"encoding\"):\n",
    "    bidirectional_concat_model = load_model(MODEL_FILEPATH)\n",
    "    print(\"Model is loaded from a pretrained model\")\n",
    "\n",
    "elif(EXECUTION_MODE == \"construct_new_model\"):\n",
    "    \n",
    "    adam = Adam(lr=0.01)\n",
    "    \n",
    "    inp = Input(shape=(MAX_LENGTH_FOR_TRAINING, ENCODING_SIZE_FOR_TRAINING))\n",
    "    print('our input shape is ',(MAX_LENGTH_FOR_TRAINING, ENCODING_SIZE_FOR_TRAINING) )\n",
    "    x = Bidirectional( LSTM(128) , input_shape=(MAX_LENGTH_FOR_TRAINING, 1),  merge_mode='concat' )(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    if NUM_OF_LEARNING_OBJECTIVES == \"all_in_binary_categories\":\n",
    "        output = Dense(381, activation ='sigmoid')(x)\n",
    "        bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "        bidirectional_concat_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    elif NUM_OF_LEARNING_OBJECTIVES == \"all_used_in_binary_categories\":\n",
    "        output = Dense(len(LEARNING_OBJECTIVES_IN_USED), activation ='sigmoid')(x)\n",
    "        bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "        bidirectional_concat_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    else:\n",
    "        output = Dense(NUM_OF_LEARNING_OBJECTIVES, activation ='linear')(x)\n",
    "        bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "        bidirectional_concat_model.compile(loss='mse', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "    bidirectional_concat_model.compile(loss='mse', optimizer=adam, metrics=['accuracy'])\n",
    "    print(\"Model is newly constructed\")\n",
    "    \n",
    "bidirectional_concat_model.summary()\n",
    "pprint(bidirectional_concat_model.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "#     EPOCH_SIZE = int(1550/BATCH_SIZE)\n",
    "EPOCH_SIZE = 1\n",
    "\n",
    "training_batch_generator = batch_generator(\"./encoded/{}/train/\".format(FILE_SUFFIX), BATCH_SIZE)\n",
    "testing_batch_generator = batch_generator(\"./encoded/{}/test/\".format(FILE_SUFFIX), int(BATCH_SIZE/4))\n",
    "\n",
    "filepath=\"checkpoints/Learning_objectives-LSTM_bidirectional_%s-weights-{epoch:02d}-{loss:.4f}.hdf5\" % FILE_SUFFIX\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', save_best_only=True, mode='min') # , verbose=1\n",
    "reduce_LR = ReduceLROnPlateau(monitor='loss',factor = 0.9, patience=3,cooldown=2, min_lr = 0.00001)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=50) #, min_delta=0.0001)\n",
    "callbacks_list = [checkpoint ]# reduce_LR, early_stopping] # \n",
    "\n",
    "history = bidirectional_concat_model.fit_generator(\n",
    "    training_batch_generator,\n",
    "    steps_per_epoch=BATCH_SIZE,\n",
    "    validation_data=testing_batch_generator,\n",
    "    validation_steps=BATCH_SIZE/4,\n",
    "    epochs=EPOCH_SIZE,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "\n",
    "list_of_histories.append(history)\n",
    "\n",
    "plot_train(list_of_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bidirectional_concat_model.save(MODEL_FILEPATH)\n",
    "\n",
    "if False:\n",
    "    # serialize model to JSON\n",
    "    model_json = bidirectional_concat_model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    bidirectional_concat_model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "def decode_learning_objectives_from_regression(encoded_batch):\n",
    "    encoded_batch = np.array(encoded_batch)\n",
    "    encoded_batch = encoded_batch * 380    \n",
    "    encoded_batch = encoded_batch.astype(\"uint8\");\n",
    "    return encoded_batch\n",
    "\n",
    "def decode_learning_objectives_from_binary_categories(encoded_batch):\n",
    "    encoded_batch = list(map(\n",
    "        lambda encoded: [i for i, x in enumerate(encoded) if x >= 0.05],\n",
    "        encoded_batch\n",
    "    ))\n",
    "    return encoded_batch\n",
    "\n",
    "def decode_learning_objectives_in_used_from_binary_categories(encoded_batch):\n",
    "    encoded_batch = list(map(\n",
    "        lambda encoded: [\n",
    "            LEARNING_OBJECTIVES_IN_USED[i] for i, x in enumerate(encoded) if x >= 0.0001\n",
    "        ],\n",
    "        encoded_batch\n",
    "    ))\n",
    "    return encoded_batch\n",
    "    \n",
    "def predict_with_model(model):\n",
    "    test, actual = next(testing_batch_generator)\n",
    "    test = test[0:5]\n",
    "    actual = actual[0:5]\n",
    "\n",
    "    preds = model.predict(test)\n",
    "        \n",
    "    if NUM_OF_LEARNING_OBJECTIVES == \"all_in_binary_categories\":\n",
    "        # all Los in binary categories\n",
    "        preds = decode_learning_objectives_from_binary_categories(preds)\n",
    "        actual = decode_learning_objectives_from_binary_categories(actual)\n",
    "    elif NUM_OF_LEARNING_OBJECTIVES == \"all_used_in_binary_categories\":\n",
    "        # all Los in binary categories\n",
    "        preds = decode_learning_objectives_in_used_from_binary_categories(preds)\n",
    "        actual = decode_learning_objectives_in_used_from_binary_categories(actual)\n",
    "    else:\n",
    "        # LOs as regression\n",
    "        preds = decode_learning_objectives_from_regression(preds)\n",
    "        actual = decode_learning_objectives_from_regression(actual)\n",
    "    \n",
    "    actual = [str(act) for act in actual]\n",
    "    preds = [str(pred) for pred in preds]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Actual': actual,\n",
    "        'Predicted': preds\n",
    "    })\n",
    "    print(df)\n",
    "\n",
    "    \n",
    "predict_with_model(bidirectional_concat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
