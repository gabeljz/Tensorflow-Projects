{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input-Output\n",
    "import json\n",
    "from pprint import pprint\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "# Data manipulation\n",
    "# WARNING, sympy has a re() function which will conflicts with the regex library - \"re\"\n",
    "# it is safer to namespace the whole sympy library as \"sp\"\n",
    "# import sympy as sp \n",
    "import re\n",
    "import random\n",
    "\n",
    "# Programmed on\n",
    "# Keras version 2.0.8\n",
    "# Tensorflow version 1.3.0\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXECUTION_MODE = \"construct_new_model\" # to contruct a new model\n",
    "EXECUTION_MODE = \"load_pretrained_model\" # to load a saved model that was trained previously\n",
    "\n",
    "# OUTPUT_TYPES = \"chapter\"\n",
    "OUTPUT_TYPES = \"learning_objectives\"\n",
    "\n",
    "LEARNING_OBJECTIVES_ENCODING = \"first_used_LO-one_hot\"\n",
    "\n",
    "NUM_OF_LEARNING_OBJECTIVES = 2\n",
    "\n",
    "if OUTPUT_TYPES == \"chapter\":\n",
    "    FILENAME = \"model-{}\".format(OUTPUT_TYPES)\n",
    "else:\n",
    "    FILENAME = \"model-{}-{}-{}\".format(OUTPUT_TYPES, LEARNING_OBJECTIVES_ENCODING, NUM_OF_LEARNING_OBJECTIVES)\n",
    "\n",
    "# Global variables that are meant to be updated by the functions\n",
    "# Before removing brackets was 356\n",
    "# After removing brackets is 221\n",
    "MAX_LENGTH = 0;\n",
    "\n",
    "# Constants for training\n",
    "MAX_LENGTH_FOR_TRAINING = 221\n",
    "ENCODING_SIZE_FOR_TRAINING = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click to jump to [\"5. Load encoded data and train Model\"](http://localhost:8888/notebooks/projects/Workings_to_Meta/Identify_learning_objectives_from_workings.ipynb#5.-Load-encoded-data-and-train-Model)\n",
    "\n",
    "# 1. Identify Learning Objectives from Student's Working\n",
    "\n",
    "## 1.1. Goal of this project\n",
    "\n",
    "This project is a proof-of-concept for possible advancement in the capabilites of the product that I am working on.\n",
    "\n",
    "The product is a online platform which can let students attempt Maths questions with full workings, and the system will automatically mark every step as correct or wrong.\n",
    "\n",
    "Currently, the system can mark a working to a question as either fully correct or highlight the mistakes to the students.\n",
    "\n",
    "However, if a working has been marked as fully correct, the system can not detect if the student had achieved the learnings objectives that had been set in the question.\n",
    "\n",
    "Every question has been authored with a **set of learning objectives** and the author had provided **a full working that achieved all the learning objectives**.\n",
    "\n",
    "The learning objectives used in the product are derived from MOE's learning experience for Secondary School Mathematics.\n",
    "\n",
    "* https://www.moe.gov.sg/docs/default-source/document/education/syllabuses/sciences/files/mathematics-syllabus-sec-1-to-4-express-n(a)-course.pdf#page=34\n",
    "\n",
    "The goal of this project is to indentify the learning objectives that has been achieved in a working, hence when the student enter his working, the model can be used to predict the learning objectives achieved by him, and match against the learning objectives that was set for the question by the author.\n",
    "\n",
    "\n",
    "\n",
    "## 1.2. Conclusion\n",
    "\n",
    "Using a BiDirectional LSTM model to train on the current set of data results in one of two problems:\n",
    "\n",
    "1. Overfitting when trained to output binary categories\n",
    "2. Apparently random predictions when trained to output as regression.\n",
    "\n",
    "This project proves that trying to predict the learning objectives achieved by the students based on his whole working is not feasible.\n",
    "\n",
    "### 1.2.1. Update after trying \"2.0. One-hot encoding for all Chapters\"\n",
    "\n",
    "Instead of trying to train of multiple labels per sample, the models achieve much better accuracies training on a single label.\n",
    "\n",
    "* **> 80%** accuracy on predicting chapter\n",
    "* **> 50%** accuracy on predicting a single learning objective\n",
    "\n",
    "This demostrates that it is feasible to encode a Math expression so that a model can use it as input and achieve meaningful prediction.\n",
    "\n",
    "It is especially promising because the data used in this project is only one type of data that are assoicated to every questions. There other data in the system that can be associated to the questions and workings which can be used to enchance these models further.\n",
    "\n",
    "Further work will be done to explore using machine learning to advance the capaibilities of the product.\n",
    "\n",
    "## 1.3. Further Work\n",
    "\n",
    "1. Try a All-You-Need-Is-Attention model on the same input and output data.\n",
    "2. Try to encode the input data into a 2D picture and try to use a CNN model to train on it.\n",
    "3. Try to build a model to detect the types of equations in a working.\n",
    "    * While predicting the learning objectives achieved in a whole working might not be feasible,\n",
    "    * it might still be possible to predict the type of equations on the individual equations in the whole working.\n",
    "    * Then another model can use the predicted types of equations,\n",
    "    * to further predict the learning objectives achieved.\n",
    "    * However, presently, we do not have enough labelled equations to train on this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Output encoding and model\n",
    "\n",
    "There are 380 possible learning objectives in the system. Their identifiers start from 1 to 380.\n",
    "\n",
    "Every question has a set of learning objectives, which can have one or more elements.\n",
    "\n",
    "The learning objectives in each questions have been arranged from highest to lowest priority.\n",
    "\n",
    "Three different encoding scheme had been explored, and each scheme will require a different output layer in the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_OBJECTIVES_IN_USED = [\n",
    "    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 19, 20,\n",
    "    21, 22, 23, 24, 25, 27, 28, 31, 35, 37, 38, 39, 40, 42, 44, 45,\n",
    "    55, 56, 57, 58, 59, 60, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87,\n",
    "    88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102,\n",
    "    104, 105, 107, 108, 109, 112, 113, 114, 115, 138, 141, 142, 143,\n",
    "    144, 145, 158, 159, 160, 161, 163, 182, 183, 184, 185, 216, 217,\n",
    "    218, 219, 220, 223, 224, 226, 280, 281, 282, 283, 285, 289, 290,\n",
    "    291, 292, 293, 294, 295, 296, 297, 298, 323\n",
    "]\n",
    "\n",
    "LEARNING_OBJECTIVES_COUNTER = np.zeros(381, dtype=np.int_).tolist()\n",
    "\n",
    "def encode_learning_objectives(LOs):\n",
    "    global LEARNING_OBJECTIVES_COUNTER\n",
    "    \n",
    "    if len(LOs) == 0:\n",
    "        LOs = [0]\n",
    "    \n",
    "    for LO in LOs:\n",
    "        LEARNING_OBJECTIVES_COUNTER[LO] += 1\n",
    "        \n",
    "    encoded = {\n",
    "        \"first_LO-one_hot\" : [],\n",
    "        \"all_LOs-multi_hot\" : [],\n",
    "        \"first_used_LO-one_hot\" : [],\n",
    "        \"all_used_LOs-multi_hot\" : [],\n",
    "        \"regression\" : []\n",
    "    }\n",
    "    \n",
    "    learning_objectives = np.zeros(381, dtype=np.int_)\n",
    "    learning_objectives[LOs[0]] = 1\n",
    "    encoded[\"first_LO-one_hot\"] = learning_objectives.tolist()\n",
    "    \n",
    "    learning_objectives[LOs] = 1\n",
    "    encoded[\"all_LOs-multi_hot\"] = learning_objectives.tolist()\n",
    "    \n",
    "    learning_objectives = np.zeros(len(LEARNING_OBJECTIVES_IN_USED), dtype=np.int_)\n",
    "    for idx, LO in enumerate(LOs):\n",
    "        learning_objectives[LEARNING_OBJECTIVES_IN_USED.index(LO)] = 1\n",
    "        if idx==0:\n",
    "            encoded[\"first_used_LO-one_hot\"] = learning_objectives.tolist()\n",
    "            \n",
    "    encoded[\"all_used_LOs-multi_hot\"] = learning_objectives.tolist()\n",
    "    \n",
    "    learning_objectives = [x/380 for x in LOs]\n",
    "    encoded[\"regression\"] = learning_objectives\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0. One-hot encoding for all Chapters\n",
    "\n",
    "The earlier results on training for learning objectives were not great.\n",
    "\n",
    "Hence to try to set a more achieveable goal, I had switched to traiing the model to predicts the chapter that a working is based on.\n",
    "\n",
    "There are only 15 chapters, as compared to 380 learning objectives, hence this should be an easier goal.\n",
    "\n",
    "The chapter is simply one-hot encoded.\n",
    "\n",
    "The output layer of the Model is then configured as such:\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output = Dense(15, activation ='softmax')(x)\n",
    "bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "bidirectional_concat_model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "This model achieve **>80% training and validation accuracies**.\n",
    "\n",
    "This suggests that the input encoding is functional and efficient.\n",
    "\n",
    "Hence I went back to tweak the model for [\"2.2. Binary Categories for used Learning Objectives\"](http://localhost:8888/notebooks/projects/Workings_to_Meta/Identify_learning_objectives_%26_chapters_from_workings.ipynb#2.2.-Binary-Categories-for-used-Learning-Objectives). See the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def encode_chapter(chapter, size):\n",
    "    encoded = np.zeros(size, dtype=np.int_)\n",
    "    encoded[chapter] = 1\n",
    "    return encoded.tolist()\n",
    "\n",
    "print(encode_chapter(7, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Binary Categories for all Learning Objectives\n",
    "\n",
    "The obvious choice for encoding the output is binary categories for all 380 learning objectives.\n",
    "\n",
    "The output layer of the Model is then configured as such:\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output = Dense(381, activation ='sigmoid')(x)\n",
    "bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "bidirectional_concat_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "After some training, it was apparent that there are a few problems\n",
    "\n",
    "1. The training time is too long.\n",
    "2. The output is always 21, suggesting that the Model is over-fitting.\n",
    "3. The accuracy has plateaued at 0.1065, and fluctuates a little.\n",
    "\n",
    "But most of the learning objectives are not used in the system, and hence are always set to 0.\n",
    "\n",
    "Hence the learning objectives was trimmed by the encoding scheme - \"Binary Categories for used Learning Objectives\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(381,)\n",
      "[0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "learning_objectives = [1, 3, 5, 7, 101, 216, 323]\n",
    "encoded = encode_learning_objectives(learning_objectives)\n",
    "print(np.array(encoded[\"all_LOs-multi_hot\"]).shape)\n",
    "print(encoded[\"all_LOs-multi_hot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Binary Categories for used Learning Objectives\n",
    "\n",
    "Learning objectives that are not used in the system are removed from the training output.\n",
    "\n",
    "This reduces the possible output from 380 to 132.\n",
    "\n",
    "The output layer of the Model is then configured as such:\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output = Dense(len(LEARNING_OBJECTIVES_IN_USED), activation ='sigmoid')(x)\n",
    "bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "bidirectional_concat_model.compile(loss='binary_\"crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "After some training, it was apparent that there are a few problems\n",
    "\n",
    "1. The output is always 21, suggesting that the Model is over-fitting.\n",
    "2. The accuracy has plateaued at around 0.1065, and fluctuates a little.\n",
    "\n",
    "To totally avoid over-fitting,  the output is forced to become a single value, hence the next encoding scheme is used - \"Linear Regression for the first N Learning Objectives\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113,)\n",
      "[0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "learning_objectives = [1, 3, 5, 7, 101, 216, 323]\n",
    "encoded = encode_learning_objectives(learning_objectives)\n",
    "print(np.array(encoded[\"all_used_LOs-multi_hot\"]).shape)\n",
    "print(encoded[\"all_used_LOs-multi_hot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Update after trying \"2.0. One-hot encoding for all Chapters\"\n",
    "\n",
    "In this model, I reduced the learning objectives for every question to its first learning objective, which should have the highest priority in that question.\n",
    "\n",
    "Now, instead of having multiple labels, there is only one label per input sample.\n",
    "\n",
    "The output layer of the Model is then configured as such:\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output = Dense(len(LEARNING_OBJECTIVES_IN_USED), activation ='softmax')(x)\n",
    "bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "bidirectional_concat_model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "After 2 epochs of training, this model achieves **>50% training and validation accuracies**.\n",
    "\n",
    "This is a much better result than earlier.\n",
    "\n",
    "After some further tweaking and reading online, I realised that a multiple-hot label model is known to be hard to train.\n",
    "\n",
    "* [How to deal with a multi label classification of sparse vector](https://github.com/fchollet/keras/issues/2578)\n",
    "\n",
    "In outputs where most of the values are **zeros**, and the hot values are sparse, the model has a strong bias to output everything to zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Linear Regression for the first N Learning Objectives\n",
    "\n",
    "Only the first Learning Objectives, which is of the highest priority for the question, is used as the output.\n",
    "\n",
    "The first learning objective is normalized by dividing by 380 to result in a float.\n",
    "\n",
    "The Model is trained and used to predict this floating value, which will be decoded by multiplying by 380.\n",
    "\n",
    "The output layer of the Model is then configured as such:\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output = Dense(NUM_OF_LEARNING_OBJECTIVES, activation ='linear')(x)\n",
    "bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "bidirectional_concat_model.compile(loss='mse', optimizer=adam, metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "After some training, it was apparent that there are a few problems\n",
    "\n",
    "1. The predicted output is random, and has no observable pattern with the actual target.\n",
    "    * While the learning objectives are indexed in such a way that they are similiar to their neighbours, this suggest that the similiarity of neighbouring learning objectives are not detectable by the Model.\n",
    "    * The training input do not contain sufficient informations. Some critical signals are missing from the input, and can not be encoded into a stream of values easily.\n",
    "2. The training accuracy has plateaued at 0.0187.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n",
      "[0.002631578947368421, 0.007894736842105263, 0.013157894736842105, 0.018421052631578946, 0.2657894736842105, 0.5684210526315789, 0.85]\n"
     ]
    }
   ],
   "source": [
    "learning_objectives = [1, 3, 5, 7, 101, 216, 323]\n",
    "encoded = encode_learning_objectives(learning_objectives)\n",
    "print(np.array(encoded[\"regression\"]).shape)\n",
    "print(encoded[\"regression\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Input encoding\n",
    "\n",
    "One of the biggest challenges in this project initially, is encoding of the input, as the input data are workings that are made up of Mathematical symbols and operator.\n",
    "\n",
    "While the students will actually enter their workings in Latex, these Latex working had been converted to a format that is compatible with a Computer Algebra System for processing by the system.\n",
    "\n",
    "These converted workings will be used as the input to this project.\n",
    "\n",
    "## 3.1. One-hot encoding\n",
    "\n",
    "An obvious choice in encoding scheme is one hot encoding.\n",
    "\n",
    "The converted workings contain Mathematical operators and symbols.\n",
    "\n",
    "Among all the workings provided by the author, **44 classes of operators and symbols** have been identified.\n",
    "\n",
    "Each of these classes are been assigned with a one-hot encoding.\n",
    "\n",
    "Then the whole working is tokenised and converted to this one-hot encoding.\n",
    "\n",
    "With this encoding scheme, the longest sequnce has a length of 356 encoded tokens, and each encoded token has a size of 44."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 47)\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "TOKENS_ENCODING_LIST = [\n",
    "    'bracket_depth', 'newline',\n",
    "    '(', ')',\n",
    "    '=', '<=', '>=', '<', '>',\n",
    "    '+', '-', '*', '/', '^',\n",
    "    '&%', '&:', '&`*', '&p`', '&`',\n",
    "    'sin', 'cos', 'tan', 'sqrt', '%pi',\n",
    "    'qn_var', 'qn_var_para', 'declared_var', 'declared_var_para', 'ambiguous', \n",
    "    'dollar', 'cent', 'degree',\n",
    "    'hour', 'minute', 's',\n",
    "    'l', 'g', 'm',\n",
    "    'k', 'c', 'milli',\n",
    "    'whole', 'decimal', 'mix_frac',\n",
    "    'arbi_unit', 'arbi_unit_para',\n",
    "    'func',        \n",
    "]\n",
    "ENCODING_SIZE = len(TOKENS_ENCODING_LIST);\n",
    "ENCODING_COUNTER = np.zeros(ENCODING_SIZE, dtype=np.int_)\n",
    "    \n",
    "def encode_mx_token(token):\n",
    "    global ENCODING_COUNTER\n",
    "    encoded_mx_token = np.zeros(len(TOKENS_ENCODING_LIST), dtype=np.int_)\n",
    "    idx = TOKENS_ENCODING_LIST.index(token)\n",
    "    encoded_mx_token[idx] = 1\n",
    "    ENCODING_COUNTER[idx] += 1\n",
    "    return encoded_mx_token.tolist()\n",
    "\n",
    "def encode_by_fixed_str(enable, mx_str, needle, encoded_list):\n",
    "    if enable and mx_str[0:len(needle)] == needle:\n",
    "        mx_str = mx_str[len(needle):]\n",
    "        encoded_list.append(encode_mx_token(needle))\n",
    "        enable = False\n",
    "    return enable, mx_str, encoded_list\n",
    "\n",
    "def encode_by_regex(enable, mx_str, regex_seed, encoded_list, encoding):\n",
    "    regex = r'' + regex_seed\n",
    "    if enable and re.match(regex, mx_str):\n",
    "        m = re.match(regex, mx_str)\n",
    "        m = m.group(1)\n",
    "\n",
    "        encoded = encode_mx_token(encoding)\n",
    "\n",
    "        if encoding+\"_para\" in TOKENS_ENCODING_LIST:\n",
    "            para = re.search(r'\\d+\\.?\\d*', m)\n",
    "            if para is not None:\n",
    "                para = float(para.group(0))\n",
    "                encoded[TOKENS_ENCODING_LIST.index(encoding+\"_para\")] = para/10\n",
    "                ENCODING_COUNTER[TOKENS_ENCODING_LIST.index(encoding+\"_para\")] = \\\n",
    "                max(ENCODING_COUNTER[TOKENS_ENCODING_LIST.index(encoding+\"_para\")], para)\n",
    "\n",
    "        encoded_list.append(encoded)\n",
    "        mx_str = mx_str[len(m):]\n",
    "        enable = False\n",
    "    return enable, mx_str, encoded_list\n",
    "\n",
    "def encode_mx_string(mx_str):\n",
    "    encoded_list = []\n",
    "    remaining = \"\"\n",
    "    \n",
    "#     print(mx_str)\n",
    "\n",
    "    while(len(mx_str) > 0):\n",
    "        enable = True;\n",
    "\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"newline\", encoded_list)\n",
    "\n",
    "        # Customised Function with Brackets and commands\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(T_ud\\([^)]+\\))\", encoded_list, \"func\")\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(mix_frac\\(\\d+\\s*,\\s*\\d+\\s*\\/\\s*\\d+\\s*\\))\", encoded_list, \"mix_frac\")\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(select_symbol\\([^)]+\\))\", encoded_list, \"ambiguous\")\n",
    "\n",
    "        # Brackets\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"(\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \")\", encoded_list)\n",
    "\n",
    "        # Relational operator\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \">=\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"<=\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"=\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \">\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"<\", encoded_list)\n",
    "\n",
    "        # Arithmetic operator\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"+\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"-\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"*\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"/\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"^\", encoded_list)\n",
    "\n",
    "        # Custome Arithmetic operator\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"&%\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"&:\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"&p`\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"&`*\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"&`\", encoded_list)\n",
    "\n",
    "        # Standard functions\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"sin\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"cos\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"tan\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"sqrt\", encoded_list)\n",
    "\n",
    "        # Constant\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"%pi\", encoded_list)\n",
    "\n",
    "        # variable\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(d\\d+)\", encoded_list, \"declared_var\")\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(o\\d+)\", encoded_list, \"qn_var\")\n",
    "\n",
    "        # Arbitrary unit\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(u\\d+)\", encoded_list, \"arbi_unit\")\n",
    "\n",
    "        # numeric\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(\\d+\\.\\d+)\", encoded_list, \"decimal\")\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(\\d+)\", encoded_list, \"whole\")\n",
    "\n",
    "        # Units\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"dollar\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"cent\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"degree\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"hour\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"minute\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"s\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"l\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"g\", encoded_list)\n",
    "\n",
    "        # Prefixes\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"k\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"c\", encoded_list)\n",
    "        enable, mx_str, encoded_list = encode_by_regex(enable, mx_str, \"(m)\\s*&`*\", encoded_list, \"milli\")\n",
    "\n",
    "        # Unit meter\n",
    "        enable, mx_str, encoded_list = encode_by_fixed_str(enable, mx_str, \"m\", encoded_list)\n",
    "\n",
    "        if enable:\n",
    "            remaining += mx_str[0]\n",
    "            mx_str = mx_str[1:]        \n",
    "\n",
    "#     print(\"\\nencoded:\\n\", encoded_list)\n",
    "    remaining = remaining.replace(\" \", \"\")\n",
    "\n",
    "#     print(\"\\nremaining:\\n\", remaining)\n",
    "    if len(remaining) > 0:\n",
    "        raise ValueError(\"Not encoded:\", remaining)\n",
    "\n",
    "    return encoded_list\n",
    "\n",
    "test_mx_str = \"1+(2-3)*4\"\n",
    "test_encoded_list = encode_mx_string(test_mx_str)\n",
    "print(np.array(test_encoded_list).shape)\n",
    "print(np.array(test_encoded_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. One-hot encoding with 1X class parameter\n",
    "\n",
    "By representing all variables as a single class, too much information is removed in the final encoding.\n",
    "\n",
    "So for each of the variable class, a class parameter is added to the encoding.\n",
    "\n",
    "There are three types of variables used in the system, so three class parameters are added for the three variables classes.\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "TOKENS_ENCODING_LIST = [\n",
    "    ...\n",
    "    'qn_var',            # First type of variables - Variables defined by questions\n",
    "    'qn_var_para',       # Class parameter for the first type of variables\n",
    "    'declared_var',      # Second type of variables - Variables declared by students\n",
    "    'declared_var_para', # Class parameter for the second type of variables\n",
    "    'arbi_unit',         # Third type of variables - Variables which represent a unit\n",
    "    'arbi_unit_para',    # Class parameter for the third type of variables\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "The class parameter actually represent the index of the variable in the question, starting from **1**.\n",
    "\n",
    "For example, in a question where four variables are used - `[w, x, y, z]`.\n",
    "\n",
    "Then the indices of these four variables are -`[1, 2, 3, 4]`.\n",
    "\n",
    "They are then normalised by dividing by 10 and assigned to the class parameters - `[0.1, 0.2, 0.3, 0.4]`.\n",
    "\n",
    "Hence each variable is represented by two values in the encoding (\"qn_var\" is idx 24 and \"qn_var_para\" is 25, before removing brackets):\n",
    "\n",
    "* `encoded[24:26] = [1, 0.1]` is for \"w\"\n",
    "* `encoded[24:26] = [1, 0.2]` is for \"x\"\n",
    "* `encoded[24:26] = [1, 0.3]` is for \"y\"\n",
    "* `encoded[24:26] = [1, 0.4]` is for \"z\"\n",
    "\n",
    "With this encoding scheme, each the size of encoded token is increased from 44 to 47."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 47)\n",
      "All question variables parameters: [0.1, 0, 0, 0.2, 0, 0.3, 0, 0, 0.4]\n",
      "Encoding for \"w\" [1, 0.1]\n",
      "Encoding for \"x\" [1, 0.2]\n",
      "Encoding for \"y\" [1, 0.3]\n",
      "Encoding for \"z\" [1, 0.4]\n",
      "[[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.1  0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. ]\n",
      " [ 0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.2  0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.3  0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. ]\n",
      " [ 0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.4  0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "test_mx_str = \"o1+(o2-o3)*o4\" # converted from latex \"w+(x-y)*z\"\n",
    "test_encoded_list = encode_mx_string(test_mx_str)\n",
    "\n",
    "print(np.array(test_encoded_list).shape)\n",
    "\n",
    "all_qn_var_para = [encoded[TOKENS_ENCODING_LIST.index(\"qn_var_para\")] for encoded in test_encoded_list]\n",
    "print(\"All question variables parameters:\", all_qn_var_para)\n",
    "print('Encoding for \"w\"', test_encoded_list[0][24:26])\n",
    "print('Encoding for \"x\"', test_encoded_list[3][24:26])\n",
    "print('Encoding for \"y\"', test_encoded_list[5][24:26])\n",
    "print('Encoding for \"z\"', test_encoded_list[8][24:26])\n",
    "print(np.array(test_encoded_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. One-hot encoding with depth of bracket\n",
    "\n",
    "The vanilla one-hot encoding result in a rather long sequence.\n",
    "\n",
    "In Maths, the brackets are very often used, hence it is present in all workings and might not be a useful signal for our model.\n",
    "\n",
    "However, the contents within a pair of bracket has a different schematic meaning from the content outside the brackets.\n",
    "\n",
    "For every token, the depth of its surrounding brackets are counted and encoded as a decimal with a 0.1 increment per level.\n",
    "\n",
    "This encoded depth  is then included in the encoding.\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "TOKENS_ENCODING_LIST = [\n",
    "    ...\n",
    "    'bracket_depth',\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "Then the encoded bracket tokens are removed from the encoded sequence.\n",
    "\n",
    "With this encoding scheme, the longest sequnce is reduce from a length of 356 to a length of 221 encoded tokens.\n",
    "\n",
    "The size of each encoded token if reduced from 47 to 45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before encoding and removing bracket tokens:\n",
      "(9, 47)\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0]]\n",
      "After encoding and removing bracket tokens:\n",
      "(7, 45)\n",
      "[[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.1  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0. ]\n",
      " [ 0.1  0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.1  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "# Only the content within the brackets should have the new depth\n",
    "# The brackets themselves are still at the same depth as outside the bracket\n",
    "def encode_bracket_depth(encoded_list):\n",
    "    global ENCODING_COUNTER\n",
    "    depth_idx = TOKENS_ENCODING_LIST.index(\"bracket_depth\")\n",
    "    lb_indx = TOKENS_ENCODING_LIST.index(\"(\")\n",
    "    rb_indx = TOKENS_ENCODING_LIST.index(\")\")\n",
    "\n",
    "    depth = 0\n",
    "\n",
    "    for encoded in encoded_list:\n",
    "        if encoded[lb_indx] == 1:\n",
    "            encoded[depth_idx] = depth/10\n",
    "            depth += 1\n",
    "        elif encoded[rb_indx] == 1:\n",
    "            depth -= 1\n",
    "            encoded[depth_idx] = depth/10\n",
    "        else:\n",
    "            encoded[depth_idx] = depth/10\n",
    "\n",
    "        ENCODING_COUNTER[depth_idx] = max(ENCODING_COUNTER[depth_idx], depth)\n",
    "\n",
    "    return encoded_list\n",
    "\n",
    "def change_tokens_to_None(encoded_list, token_indices):    \n",
    "    for token_idx in token_indices:\n",
    "        encoded_indices = [i for i, encoded in enumerate(encoded_list) if encoded[token_idx] == 1]\n",
    "        for idx in encoded_indices:\n",
    "            encoded_list[int(idx)] = [None] * len(encoded_list[idx])\n",
    "        encoded_indices = [i for i, encoded in enumerate(encoded_list) if encoded[token_idx] == 1]\n",
    "        for encoded in encoded_list:\n",
    "            encoded[token_idx] = None\n",
    "    return encoded_list\n",
    "\n",
    "def purge_None(encoded_list):\n",
    "    encoded_indices = [i for i, encoded in enumerate(encoded_list) if all(v is None for v in encoded)]\n",
    "    for idx in sorted(encoded_indices, reverse=True):\n",
    "        del encoded_list[idx]\n",
    "    encoded_list = [list(filter(lambda x: x is not None, encoded)) for encoded in encoded_list]\n",
    "    return encoded_list\n",
    "\n",
    "test_mx_str = \"1+(2-3)*4\"\n",
    "test_encoded_list = encode_mx_string(test_mx_str)\n",
    "print(\"Before encoding and removing bracket tokens:\")\n",
    "print(np.array(test_encoded_list).shape)\n",
    "print(np.array(test_encoded_list))\n",
    "\n",
    "# Encode bracket depth \n",
    "test_encoded_list = encode_bracket_depth(test_encoded_list)\n",
    "# Remove brackets\n",
    "lb_idx = TOKENS_ENCODING_LIST.index(\"(\")\n",
    "rb_idx = TOKENS_ENCODING_LIST.index(\")\")\n",
    "token_indices = [lb_idx, rb_idx]\n",
    "test_encoded_list = change_tokens_to_None(test_encoded_list, token_indices)\n",
    "test_encoded_list = purge_None(test_encoded_list)\n",
    "print(\"After encoding and removing bracket tokens:\")\n",
    "print(np.array(test_encoded_list).shape)\n",
    "print(np.array(test_encoded_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. One-hot encoding with depth of bracket and 1X class parameter, no less-used classes\n",
    "\n",
    "The \"sin\", \"cos\", and \"tan\" are only used 6 times each in the training samples.\n",
    "\n",
    "Hence they are removed from the encoding.\n",
    "\n",
    "With this encoding scheme, the longest sequence is still at a length of 221 encoded tokens, as it does not contain any of the \"sin\", \"cos\", and \"tan\" tokens.\n",
    "\n",
    "Each encoded token has been reduced from a size of 45 to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 42)\n",
      "[[ 0.1  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.1  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0. ]\n",
      " [ 0.1  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.1  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   1.   0.1  0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "test_mx_str = \"sin(%pi)+cos(60&`(degree))+tan(o1)\" # converted from latex \"\\sin(\\pi)+\\cos(60\\degree)-\\tan(x)\"\n",
    "test_encoded_list = encode_mx_string(test_mx_str)\n",
    "\n",
    "# Encode bracket depth \n",
    "test_encoded_list = encode_bracket_depth(test_encoded_list)\n",
    "# Remove brackets\n",
    "lb_idx = TOKENS_ENCODING_LIST.index(\"(\")\n",
    "rb_idx = TOKENS_ENCODING_LIST.index(\")\")\n",
    "token_indices = [lb_idx, rb_idx]\n",
    "test_encoded_list = change_tokens_to_None(test_encoded_list, token_indices)\n",
    "\n",
    "# Remove trigo function\n",
    "sin_idx = TOKENS_ENCODING_LIST.index(\"sin\")\n",
    "cos_idx = TOKENS_ENCODING_LIST.index(\"cos\")\n",
    "tan_idx = TOKENS_ENCODING_LIST.index(\"tan\")\n",
    "token_indices = [sin_idx, cos_idx, tan_idx]\n",
    "test_encoded_list = change_tokens_to_None(test_encoded_list, token_indices)\n",
    "\n",
    "test_encoded_list = purge_None(test_encoded_list)\n",
    "\n",
    "print(np.array(test_encoded_list).shape)\n",
    "print(np.array(test_encoded_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_list_of_histories(list_of_histories, filepath):\n",
    "    saveable_histories = []\n",
    "    for history in list_of_histories:\n",
    "        try:\n",
    "            del history.model\n",
    "        except:\n",
    "            pass\n",
    "        saveable_histories.append(history)\n",
    "\n",
    "    with open(filepath, 'wb') as file_pi:\n",
    "        pickle.dump(saveable_histories, file_pi)\n",
    "        \n",
    "def load_list_of_histories(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'rb') as file_pi:\n",
    "            saveable_histories = pickle.load(file_pi)\n",
    "    except:\n",
    "        saveable_histories = []\n",
    "    return saveable_histories\n",
    "\n",
    "def plot_train(list_of_histories):\n",
    "    try:\n",
    "        if 'acc' in list_of_histories[0].history:\n",
    "            meas='acc'\n",
    "            loc='lower right'\n",
    "        else:\n",
    "            meas='loss'\n",
    "            loc='upper right'\n",
    "\n",
    "        train_meas = []\n",
    "        val_meas = []\n",
    "\n",
    "        for hist in list_of_histories:\n",
    "            train_meas = train_meas + hist.history[meas]\n",
    "            val_meas = val_meas + hist.history['val_'+meas]\n",
    "\n",
    "        plt.plot(train_meas)\n",
    "        plt.plot(val_meas)\n",
    "        plt.title('model '+meas)\n",
    "        plt.ylabel(meas)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc=loc)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Main encoding function: encode_and_save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def encode_and_save_file(filepath, idx):\n",
    "    global MAX_LENGTH\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    with open(filepath) as json_data:\n",
    "\n",
    "        raw_data = json.load(json_data)\n",
    "        \n",
    "        level = raw_data['level']\n",
    "        chapter = raw_data['chapter']\n",
    "        section = raw_data['section']\n",
    "        learning_objectives_raw = raw_data['learning_objectives']\n",
    "        working_in_mx = raw_data['mx_working']\n",
    "        \n",
    "        chapter_encoded = encode_chapter(chapter-1, 15)\n",
    "        \n",
    "        learning_objectives = encode_learning_objectives(learning_objectives_raw)       \n",
    "\n",
    "        encoded_mx_working = \"newline\".join(working_in_mx);\n",
    "        encoded_mx_working = encode_mx_string(encoded_mx_working)\n",
    "        \n",
    "        # Encode bracket's depth\n",
    "        encoded_mx_working = encode_bracket_depth(encoded_mx_working)\n",
    "        # Remove brackets\n",
    "        lb_idx = TOKENS_ENCODING_LIST.index(\"(\")\n",
    "        rb_idx = TOKENS_ENCODING_LIST.index(\")\")\n",
    "        token_indices = [lb_idx, rb_idx]\n",
    "        encoded_mx_working = change_tokens_to_None(encoded_mx_working, token_indices)\n",
    "\n",
    "        # Remove trigo function\n",
    "        sin_idx = TOKENS_ENCODING_LIST.index(\"sin\")\n",
    "        cos_idx = TOKENS_ENCODING_LIST.index(\"cos\")\n",
    "        tan_idx = TOKENS_ENCODING_LIST.index(\"tan\")\n",
    "        token_indices = [sin_idx, cos_idx, tan_idx]\n",
    "        encoded_mx_working = change_tokens_to_None(encoded_mx_working, token_indices)\n",
    "\n",
    "        encoded_mx_working = purge_None(encoded_mx_working)\n",
    "        MAX_LENGTH = max(MAX_LENGTH or 0, len(encoded_mx_working))\n",
    "\n",
    "    qn = re.search(r'qn_(\\d+)', filename);\n",
    "    qn = int(qn.group(1))\n",
    "\n",
    "    filepath = \"./encoded/{}\".format(filename)\n",
    "    with open(filepath, \"w\") as outfile:\n",
    "        encoded_data = {\n",
    "            'idx' : idx,\n",
    "            'qn' : qn,\n",
    "            'level' : level,\n",
    "            'chapter' : chapter_encoded,\n",
    "            'section' : section,\n",
    "            'learning_objectives' : learning_objectives,\n",
    "            'mx_working' : encoded_mx_working,\n",
    "        }\n",
    "        json.dump(encoded_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Maximum length of sequence:  221\n",
      "    Count              Token\n",
      "0   4      bracket_depth    \n",
      "1   24     newline          \n",
      "2   147    (                \n",
      "3   147    )                \n",
      "4   39     =                \n",
      "5   0      <=               \n",
      "6   0      >=               \n",
      "7   0      <                \n",
      "8   0      >                \n",
      "9   7      +                \n",
      "10  59     -                \n",
      "11  90     *                \n",
      "12  41     /                \n",
      "13  0      ^                \n",
      "14  3      &%               \n",
      "15  0      &:               \n",
      "16  2      &`*              \n",
      "17  4      &p`              \n",
      "18  9      &`               \n",
      "19  0      sin              \n",
      "20  0      cos              \n",
      "21  0      tan              \n",
      "22  0      sqrt             \n",
      "23  0      %pi              \n",
      "24  91     qn_var           \n",
      "25  3      qn_var_para      \n",
      "26  0      declared_var     \n",
      "27  0      declared_var_para\n",
      "28  0      ambiguous        \n",
      "29  4      dollar           \n",
      "30  0      cent             \n",
      "31  0      degree           \n",
      "32  2      hour             \n",
      "33  0      minute           \n",
      "34  4      s                \n",
      "35  0      l                \n",
      "36  0      g                \n",
      "37  6      m                \n",
      "38  2      k                \n",
      "39  0      c                \n",
      "40  0      milli            \n",
      "41  149    whole            \n",
      "42  1      decimal          \n",
      "43  5      mix_frac         \n",
      "44  0      arbi_unit        \n",
      "45  0      arbi_unit_para   \n",
      "46  0      func             \n",
      "Num of Los in used: 8\n",
      "[84, 85, 93, 95, 112, 114, 158, 161]\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 0\n",
    "ENCODING_COUNTER = np.zeros(ENCODING_SIZE, dtype=np.int_)\n",
    "LEARNING_OBJECTIVES_COUNTER = np.zeros(381, dtype=np.int_).tolist()\n",
    "\n",
    "for idx, filename in enumerate(os.listdir(\"./cleaned/\")):\n",
    "    if(filename != \".DS_Store\"):\n",
    "        encode_and_save_file(\"./cleaned/\" + filename, idx)\n",
    "\n",
    "print(\"Final Maximum length of sequence: \", MAX_LENGTH)\n",
    "\n",
    "print(pd.DataFrame({\n",
    "    'Token': TOKENS_ENCODING_LIST,\n",
    "    'Count': ENCODING_COUNTER\n",
    "}))\n",
    "\n",
    "LOs_in_used = [i for i, v in enumerate(LEARNING_OBJECTIVES_COUNTER) if v > 0];\n",
    "print(\"Num of Los in used:\", len(LOs_in_used))\n",
    "print(LOs_in_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load encoded data and train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LENGTH_FOR_TRAINING: 221\n",
      "ENCODING_SIZE_FOR_TRAINING: 42\n",
      "\n",
      "loaded 1403 samples.\n",
      "Batched Input training shape:  (1403, 221, 42)\n",
      "Batched Output training shape:  (1403, 113)\n",
      "\n",
      "loaded 354 samples.\n",
      "Batched Input testing shape:  (354, 221, 42)\n",
      "Batched Output testing shape:  (354, 113)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "def load_encoded_data(filepath):\n",
    "    with open(filepath) as json_data:\n",
    "        raw_data = json.load(json_data)\n",
    "        if OUTPUT_TYPES == \"chapter\":\n",
    "            return raw_data[\"qn\"], raw_data[\"mx_working\"] , raw_data[OUTPUT_TYPES]\n",
    "        if OUTPUT_TYPES == \"learning_objectives\":\n",
    "            return raw_data[\"qn\"], raw_data[\"mx_working\"] , raw_data[OUTPUT_TYPES][LEARNING_OBJECTIVES_ENCODING]\n",
    "\n",
    "def batch_generator(on_training):\n",
    "    batch_size = 3000 # overriding the passed in batch size\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for filename in os.listdir(\"./encoded/\"):\n",
    "\n",
    "        if(filename != \".DS_Store\"):\n",
    "            qn, x, y = load_encoded_data(\"./encoded/\" + filename)\n",
    "\n",
    "            if on_training == True and qn%5 != 0:\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "            elif on_training == False and qn%5 == 0:\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "\n",
    "    print(\"\\nloaded {num_samples} samples.\".format(num_samples=len(X)))\n",
    "    \n",
    "    if OUTPUT_TYPES == \"learning_objectives\" and LEARNING_OBJECTIVES_ENCODING == \"regression\":\n",
    "        print(\"moulding y\")\n",
    "        N = NUM_OF_LEARNING_OBJECTIVES\n",
    "        learning_objectives = LOs[0:N]\n",
    "        learning_objectives.extend([0]*(N-len(learning_objectives)))\n",
    "        for y in Y:\n",
    "            y = np.array(y)\n",
    "            y = y[0:N]\n",
    "            y.extend([0]*(N-len(y)))\n",
    "\n",
    "    for x in X:\n",
    "        x.extend([[0] * ENCODING_SIZE_FOR_TRAINING] * (MAX_LENGTH_FOR_TRAINING - len(x)))\n",
    "\n",
    "    # Capping batch size to the size of the array   \n",
    "    batch_size = min(len(X), batch_size)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    while True:\n",
    "        indices_random = random.sample(range(len(X)), batch_size)\n",
    "#         print(\"Num of samples: {} idx: \".format(len(indices_random)), indices_random[0:5])\n",
    "        yield X[indices_random], Y[indices_random]\n",
    "\n",
    "\n",
    "print(\"MAX_LENGTH_FOR_TRAINING:\", MAX_LENGTH_FOR_TRAINING)\n",
    "print(\"ENCODING_SIZE_FOR_TRAINING:\", ENCODING_SIZE_FOR_TRAINING)\n",
    "\n",
    "BATCH_SIZE = 1602\n",
    "\n",
    "training_batch_generator = batch_generator(True)\n",
    "X_train, Y_train = next(training_batch_generator)\n",
    "print(\"Batched Input training shape: \", X_train.shape)\n",
    "print(\"Batched Output training shape: \", Y_train.shape)\n",
    "\n",
    "testing_batch_generator = batch_generator(False)\n",
    "X_test, Y_test = next(testing_batch_generator)\n",
    "\n",
    "print(\"Batched Input testing shape: \", X_test.shape)\n",
    "print(\"Batched Output testing shape: \", Y_test.shape)\n",
    "\n",
    "print(Y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded from a pretrained model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 221, 42)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               175104    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 113)               29041     \n",
      "=================================================================\n",
      "Total params: 204,145\n",
      "Trainable params: 204,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "{'input_layers': [['input_3', 0, 0]],\n",
      " 'layers': [{'class_name': 'InputLayer',\n",
      "             'config': {'batch_input_shape': (None, 221, 42),\n",
      "                        'dtype': 'float32',\n",
      "                        'name': 'input_3',\n",
      "                        'sparse': False},\n",
      "             'inbound_nodes': [],\n",
      "             'name': 'input_3'},\n",
      "            {'class_name': 'Bidirectional',\n",
      "             'config': {'batch_input_shape': (None, 221, 1),\n",
      "                        'dtype': 'float32',\n",
      "                        'layer': {'class_name': 'LSTM',\n",
      "                                  'config': {'activation': 'tanh',\n",
      "                                             'activity_regularizer': None,\n",
      "                                             'bias_constraint': None,\n",
      "                                             'bias_initializer': {'class_name': 'Zeros',\n",
      "                                                                  'config': {}},\n",
      "                                             'bias_regularizer': None,\n",
      "                                             'dropout': 0.0,\n",
      "                                             'go_backwards': False,\n",
      "                                             'implementation': 0,\n",
      "                                             'kernel_constraint': None,\n",
      "                                             'kernel_initializer': {'class_name': 'VarianceScaling',\n",
      "                                                                    'config': {'distribution': 'uniform',\n",
      "                                                                               'mode': 'fan_avg',\n",
      "                                                                               'scale': 1.0,\n",
      "                                                                               'seed': None}},\n",
      "                                             'kernel_regularizer': None,\n",
      "                                             'name': 'lstm_3',\n",
      "                                             'recurrent_activation': 'hard_sigmoid',\n",
      "                                             'recurrent_constraint': None,\n",
      "                                             'recurrent_dropout': 0.0,\n",
      "                                             'recurrent_initializer': {'class_name': 'Orthogonal',\n",
      "                                                                       'config': {'gain': 1.0,\n",
      "                                                                                  'seed': None}},\n",
      "                                             'recurrent_regularizer': None,\n",
      "                                             'return_sequences': False,\n",
      "                                             'return_state': False,\n",
      "                                             'stateful': False,\n",
      "                                             'trainable': True,\n",
      "                                             'unit_forget_bias': True,\n",
      "                                             'units': 128,\n",
      "                                             'unroll': False,\n",
      "                                             'use_bias': True}},\n",
      "                        'merge_mode': 'concat',\n",
      "                        'name': 'bidirectional_3',\n",
      "                        'trainable': True},\n",
      "             'inbound_nodes': [[['input_3', 0, 0, {}]]],\n",
      "             'name': 'bidirectional_3'},\n",
      "            {'class_name': 'Dropout',\n",
      "             'config': {'name': 'dropout_3', 'rate': 0.2, 'trainable': True},\n",
      "             'inbound_nodes': [[['bidirectional_3', 0, 0, {}]]],\n",
      "             'name': 'dropout_3'},\n",
      "            {'class_name': 'Dense',\n",
      "             'config': {'activation': 'softmax',\n",
      "                        'activity_regularizer': None,\n",
      "                        'bias_constraint': None,\n",
      "                        'bias_initializer': {'class_name': 'Zeros',\n",
      "                                             'config': {}},\n",
      "                        'bias_regularizer': None,\n",
      "                        'kernel_constraint': None,\n",
      "                        'kernel_initializer': {'class_name': 'VarianceScaling',\n",
      "                                               'config': {'distribution': 'uniform',\n",
      "                                                          'mode': 'fan_avg',\n",
      "                                                          'scale': 1.0,\n",
      "                                                          'seed': None}},\n",
      "                        'kernel_regularizer': None,\n",
      "                        'name': 'dense_3',\n",
      "                        'trainable': True,\n",
      "                        'units': 113,\n",
      "                        'use_bias': True},\n",
      "             'inbound_nodes': [[['dropout_3', 0, 0, {}]]],\n",
      "             'name': 'dense_3'}],\n",
      " 'name': 'model_6',\n",
      " 'output_layers': [['dense_3', 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "if EXECUTION_MODE == \"load_pretrained_model\":\n",
    "    list_of_histories = load_list_of_histories(FILENAME + \".histories\")\n",
    "    bidirectional_concat_model = load_model(FILENAME + \".h5\")\n",
    "    print(\"Model is loaded from a pretrained model\")\n",
    "\n",
    "elif EXECUTION_MODE == \"construct_new_model\":\n",
    "    \n",
    "    list_of_histories = []\n",
    "    \n",
    "    adam = Adam(lr=0.01)\n",
    "    \n",
    "    inp = Input(shape=(MAX_LENGTH_FOR_TRAINING, ENCODING_SIZE_FOR_TRAINING))\n",
    "    print('our input shape is ',(MAX_LENGTH_FOR_TRAINING, ENCODING_SIZE_FOR_TRAINING) )\n",
    "    x = Bidirectional( LSTM(128) , input_shape=(MAX_LENGTH_FOR_TRAINING, 1),  merge_mode='concat' )(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    if OUTPUT_TYPES == \"learning_objectives\":\n",
    "        \n",
    "        if LEARNING_OBJECTIVES_ENCODING == \"all_LOs-multi_hot\":\n",
    "            output = Dense(381, activation ='softmax')(x)\n",
    "            bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "            bidirectional_concat_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        \n",
    "        elif LEARNING_OBJECTIVES_ENCODING == \"first_LO-one_hot\":\n",
    "            output = Dense(381, activation ='softmax')(x)\n",
    "            bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "            bidirectional_concat_model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "        \n",
    "        elif LEARNING_OBJECTIVES_ENCODING == \"all_used_LOs-multi_hot\":\n",
    "            output = Dense(len(LEARNING_OBJECTIVES_IN_USED), activation ='softmax')(x)\n",
    "            bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "            bidirectional_concat_model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "#             output = Dense(len(LEARNING_OBJECTIVES_IN_USED), activation ='sigmoid')(x)\n",
    "#             bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "#             bidirectional_concat_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        \n",
    "        elif LEARNING_OBJECTIVES_ENCODING == \"first_used_LO-one_hot\":\n",
    "            output = Dense(len(LEARNING_OBJECTIVES_IN_USED), activation ='softmax')(x)\n",
    "            bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "            bidirectional_concat_model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "            \n",
    "        elif LEARNING_OBJECTIVES_ENCODING == \"regression\":\n",
    "            output = Dense(NUM_OF_LEARNING_OBJECTIVES, activation ='linear')(x)\n",
    "            bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "            bidirectional_concat_model.compile(loss='mse', optimizer=adam, metrics=['accuracy'])\n",
    "            \n",
    "    elif OUTPUT_TYPES == \"chapter\":\n",
    "        output = Dense(15, activation ='softmax')(x)\n",
    "        bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "        bidirectional_concat_model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "    bidirectional_concat_model.compile(loss='mse', optimizer=adam, metrics=['accuracy'])\n",
    "    print(\"Model is newly constructed\")\n",
    "    \n",
    "bidirectional_concat_model.summary()\n",
    "pprint(bidirectional_concat_model.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "\n",
      "loaded 1403 samples.\n",
      "63/64 [============================>.] - ETA: 14s - loss: 0.0031 - acc: 0.7198\n",
      "loaded 354 samples.\n",
      "64/64 [==============================] - 964s - loss: 0.0031 - acc: 0.7206 - val_loss: 0.0059 - val_acc: 0.5028\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2/MAhQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120bb40f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCH_SIZE = 1\n",
    "\n",
    "training_batch_generator = batch_generator(True)\n",
    "testing_batch_generator = batch_generator(False)\n",
    "\n",
    "filepath=\"checkpoints/%s-weights-{epoch:02d}-{loss:.4f}.hdf5\" % FILENAME\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', save_best_only=True, mode='min') # , verbose=1\n",
    "reduce_LR = ReduceLROnPlateau(monitor='loss',factor = 0.9, patience=3,cooldown=2, min_lr = 0.00001)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=50) #, min_delta=0.0001)\n",
    "callbacks_list = [checkpoint ]# reduce_LR, early_stopping] # \n",
    "\n",
    "history = bidirectional_concat_model.fit_generator(\n",
    "    training_batch_generator,\n",
    "    steps_per_epoch=BATCH_SIZE,\n",
    "    validation_data=testing_batch_generator,\n",
    "    validation_steps=BATCH_SIZE/4,\n",
    "    epochs=EPOCH_SIZE,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "\n",
    "list_of_histories.append(history)\n",
    "plot_train(list_of_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 1s\n",
      "Model Accuracy = 0.50\n",
      "Model Loss = 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2/MAhQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1253be4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bidirectional_concat_model.save(FILENAME + \".h5\")\n",
    "save_list_of_histories(list_of_histories, FILENAME + \".histories\")\n",
    "\n",
    "x_test, y_test = next(testing_batch_generator)\n",
    "evaluation = bidirectional_concat_model.evaluate(x_test, y_test,\n",
    "          batch_size=x_test.shape[0])\n",
    "\n",
    "print('Model Accuracy = %.2f' % (evaluation[1]))\n",
    "print('Model Loss = %.2f' % (evaluation[0]))\n",
    "plot_train(list_of_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Actual Predicted\n",
      "0  [9]    [5]     \n",
      "1  [158]  [60]    \n",
      "2  [102]  [113]   \n",
      "3  [158]  [158]   \n",
      "4  [94]   [295]   \n"
     ]
    }
   ],
   "source": [
    "def decode_learning_objectives_from_regression(encoded_batch):\n",
    "    encoded_batch = np.array(encoded_batch)\n",
    "    encoded_batch = encoded_batch * 380    \n",
    "    encoded_batch = encoded_batch.astype(\"uint8\");\n",
    "    return encoded_batch\n",
    "\n",
    "def decode_learning_objectives_from_all_LOs(encoded_batch):\n",
    "    encoded_batch = list(map(\n",
    "        lambda encoded: np.argmax(encoded), # [i for i, x in enumerate(encoded) if x >= 0.05],\n",
    "        encoded_batch\n",
    "    ))\n",
    "    return encoded_batch\n",
    "\n",
    "def decode_learning_objectives_in_used_from_binary_categories(encoded_batch):\n",
    "    encoded_batch = list(map(\n",
    "        lambda encoded: [\n",
    "            LEARNING_OBJECTIVES_IN_USED[i] for i, x in enumerate(encoded) if x >= 0.0001\n",
    "        ],\n",
    "        encoded_batch\n",
    "    ))\n",
    "    return encoded_batch\n",
    "\n",
    "def decode_from_one_hot(encoded_batch):\n",
    "    encoded_batch = list(map(\n",
    "        lambda encoded: np.argmax(encoded),\n",
    "        encoded_batch\n",
    "    ))\n",
    "    return encoded_batch\n",
    "\n",
    "def decode_learning_objectives_in_used_from_index(encoded_batch):\n",
    "    encoded_batch = list(map(\n",
    "        lambda encoded: [\n",
    "            LEARNING_OBJECTIVES_IN_USED[encoded]\n",
    "        ],\n",
    "        encoded_batch\n",
    "    ))\n",
    "    return encoded_batch\n",
    "\n",
    "def predict_with_model(model):\n",
    "    test, actual = next(testing_batch_generator)\n",
    "    test = test[0:5]\n",
    "    actual = actual[0:5]\n",
    "    \n",
    "    preds = model.predict(test)\n",
    "    \n",
    "    # LEARNING_OBJECTIVES_ENCODING = \"first_LO-one_hot\"\n",
    "    # LEARNING_OBJECTIVES_ENCODING = \"all_LOs-multi_hot\"\n",
    "    # LEARNING_OBJECTIVES_ENCODING = \"first_used_LO-one_hot\"\n",
    "    # LEARNING_OBJECTIVES_ENCODING = \"all_used_LOs-multi_hot\"\n",
    "    # LEARNING_OBJECTIVES_ENCODING = \"regression\"\n",
    "\n",
    "    if OUTPUT_TYPES == \"chapter\":\n",
    "        preds = decode_from_one_hot(preds)\n",
    "        actual = decode_from_one_hot(actual)\n",
    "    else:\n",
    "        if LEARNING_OBJECTIVES_ENCODING == \"all_used_LOs-multi_hot\":\n",
    "            preds = decode_learning_objectives_in_used_from_binary_categories(preds)\n",
    "            actual = decode_from_one_hot(actual)\n",
    "        elif LEARNING_OBJECTIVES_ENCODING == \"first_used_LO-one_hot\":\n",
    "            preds = decode_from_one_hot(preds)\n",
    "            actual = decode_from_one_hot(actual)\n",
    "            preds = decode_learning_objectives_in_used_from_index(preds)\n",
    "            actual = decode_learning_objectives_in_used_from_index(actual)\n",
    "#     preds = model.predict(test)\n",
    "#     print(preds)\n",
    "        \n",
    "#     if NUM_OF_LEARNING_OBJECTIVES == \"all_in_binary_categories\":\n",
    "#         # all Los in binary categories\n",
    "#         preds = decode_learning_objectives_from_multi_hot(preds)\n",
    "#         actual = decode_learning_objectives_from_multi_hot(actual)\n",
    "#     elif NUM_OF_LEARNING_OBJECTIVES == \"all_used_in_binary_categories\":\n",
    "#         # all Los in binary categories\n",
    "#         preds = decode_learning_objectives_in_used_from_binary_categories(preds)\n",
    "#         actual = decode_learning_objectives_in_used_from_binary_categories(actual)\n",
    "#     else:        \n",
    "#         # LOs as regression\n",
    "#         preds = decode_learning_objectives_from_regression(preds)\n",
    "#         actual = decode_learning_objectives_from_regression(actual)\n",
    "    \n",
    "    actual = [str(act) for act in actual]\n",
    "    preds = [str(pred) for pred in preds]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Actual': actual,\n",
    "        'Predicted': preds\n",
    "    })\n",
    "    print(df)\n",
    "\n",
    "    \n",
    "predict_with_model(bidirectional_concat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LEARNING_OBJECTIVES_ENCODING = \"first_LO-one_hot\"\n",
    "# LEARNING_OBJECTIVES_ENCODING = \"all_LOs-multi_hot\"\n",
    "LEARNING_OBJECTIVES_ENCODING = \"first_used_LO-one_hot\"\n",
    "# LEARNING_OBJECTIVES_ENCODING = \"all_used_LOs-multi_hot\"\n",
    "# LEARNING_OBJECTIVES_ENCODING = \"regression\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
