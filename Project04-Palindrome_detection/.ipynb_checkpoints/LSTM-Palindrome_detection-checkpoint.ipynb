{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Programmed on\n",
    "# Keras version 2.0.8\n",
    "# Tensorflow version 1.3.0\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXECUTION_MODE = \"train\" # to train a new model\n",
    "EXECUTION_MODE = \"load_pretrained\" # to load a saved model that was trained previously\n",
    "FILE_SUFFIX = \"2000epouchs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Palindrome Detection\n",
    "\n",
    "This problem is chosen to demostrate if there is any advantages of Bidirectional LSTM over Unidirectional LSTM.\n",
    "\n",
    "The idea is that the same LSTM layer is run on the sequence in the original and the reverse directiions. \n",
    "\n",
    "Since a palindrome is identical in both directions, then the two outputs of the LSTM layer on both directions should also be the same.\n",
    "\n",
    "Then it will be a simple operation to compare the two LSTM output to detect if the sequence is a palindrome.\n",
    "\n",
    "# 2. Merge mode of the BiDirectional layer\n",
    "\n",
    "The outputs from LSTM in both directions should be the same.\n",
    "\n",
    "The two outputs are then merged in the BiDirectional layer, and the result is fed into a Dense layer.\n",
    "\n",
    "The different merge mode can be considered using this example both outputs:\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output = [2,5,3,7]\n",
    "```\n",
    "\n",
    "## 2.1 Concatenation merge mode\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output1 = [2,5,3,7]\n",
    "output2 = [2,5,3,7]\n",
    "merged_result = output1 + output2\n",
    "print(merged_result)\n",
    "# result is [2, 5, 3, 7, 2, 5, 3, 7]\n",
    "```\n",
    "\n",
    "Assuming that the label of a palindrome is **1**, while a non-palindrome is **0**.\n",
    "\n",
    "The Dense layer will then have to be trained with this merged result as its input, to produce the output **1**.\n",
    "\n",
    "A single layer of Dense layer might not be sufficient for this function.\n",
    "\n",
    "## 2.2 Average merge mode\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output1 = [2,5,3,7]\n",
    "output2 = [2,5,3,7]\n",
    "merged_result = [x1 - x2 for x1, x2 in zip(output1, output2)]\n",
    "print(merged_result)\n",
    "# [0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "Assuming that the label of a palindrome is **1**, while a non-palindrome is **0**.\n",
    "\n",
    "The Dense layer will then have to be trained with this merged result as its input, to produce the output **1**.\n",
    "\n",
    "A single layer of Dense layer might not be sufficient for this function.\n",
    "\n",
    "## 2.3 Difference merge mode\n",
    "\n",
    "Imagine that there is a difference merge mode.\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output1 = [2,5,3,7]\n",
    "output2 = [2,5,3,7]\n",
    "merged_result = [x1 - x2 for x1, x2 in zip(output1, output2)]\n",
    "print(merged_result)\n",
    "# merged_result is [2.0, 5.0, 3.0, 7.0]\n",
    "```\n",
    "\n",
    "Assuming that the label of a palindrome is **1**, while a non-palindrome is **0**.\n",
    "\n",
    "The Dense layer will then have to be trained with this merged result as its input, to produce the output **1**.\n",
    "\n",
    "A single layer of Dense layer might not be sufficient for this function.\n",
    "\n",
    "But, we can use the **Dense layer to simulate a difference merge mode**.\n",
    "\n",
    "If we invert the labels, such that the label of a palindrome is **0**, while a non-palindrome is **1**.\n",
    "\n",
    "And we use the \"concat\" merge mode or the \"None\" merge mode.\n",
    "\n",
    "Then the Dense layer can be trained easily to find the difference of the merged result to produce a **0**.\n",
    "\n",
    "```\n",
    "#!python\n",
    "\n",
    "output1 = [2,5,3,7]\n",
    "output2 = [2,5,3,7]\n",
    "merged_result = output1 + output2\n",
    "print(merged_result)\n",
    "# result is [2, 5, 3, 7, 2, 5, 3, 7]\n",
    "\n",
    "def Dense_function(inputs):\n",
    "    bias = 0;\n",
    "    weights = [1, 1, 1, 1, -1, -1, -1, -1]\n",
    "    weight_inputs = [x*w for x,w in zip(inputs, weights)]\n",
    "    return sum(weight_inputs) + bias\n",
    "\n",
    "output = Dense_function(merged_result)\n",
    "# output is 0\n",
    "```\n",
    "\n",
    "# 3. Training Results\n",
    "\n",
    "|LSTM size|Uni (x2)| Bi_Con| Bi_Ave| Bi_Con_Inv|\n",
    "|:--------|-------:|------:|------:|----------:|\n",
    "|16       |  0.9949| 0.9995| 0.9971|     0.9992|\n",
    "|8        |  0.9889| 0.9787| 0.9943|     0.9897|\n",
    "|4        |  0.9780|#0.9909| 0.9655|     0.9619|\n",
    "|4 @ 2000e|  0.9697|#0.8623| 0.9775|     0.9717|\n",
    "|2        |  0.9263| 0.9299| 0.8409|     0.9528|\n",
    "|1        |  0.8418| 0.8820| 0.8682|     0.8753|\n",
    "\n",
    "**Note**: The actual size of the LSTM output for the Unidirectional model is **twice** of that of the Bidirectional models.\n",
    "\n",
    "The validated accuracy of the Bidirectional concat model with LSTM(4) is very inconsistent, and it seems to be very susceptible to being \"struck\" with either extremely good accuracy or anomally bad accuracy.\n",
    "\n",
    "## 3.1. Conclusion\n",
    "\n",
    "The accuracies of the have a rather wide range between different training.\n",
    "\n",
    "The differences in accuracies of the different models are too small to make a conclusion.\n",
    "\n",
    "# 4. Other Observations\n",
    "\n",
    "In a set of random sequence, the ratio of palindromes to the total number of sequences is:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\frac{num^{\\thinspace len \\thinspace / \\thinspace 2}}{num^{\\thinspace len}}\n",
    "= \\frac{1}{num^{\\thinspace len \\thinspace / \\thinspace 2}}\n",
    "\\\\\n",
    "\\\\ &\\text{where}\n",
    "\\\\ & num \\rightarrow \\text{number of possible characters}\n",
    "\\\\ & len \\rightarrow \\text{length of the sequence}\n",
    "\\end{aligned}$$\n",
    "\n",
    "So in a set of sequences with $10$ characters, only $8.417 \\times 10^{-8}$ of them should be palindrome.\n",
    "\n",
    "In this notebook, when a model has low accuracy, most of the wrong detections seem to be **false positives**.\n",
    "\n",
    "This problem might be caused by half of the training data used for the training being palindrome.\n",
    "\n",
    "Hence the model has a bias to label a sequence as a palindrome.\n",
    "\n",
    "\n",
    "# 5. Further Work\n",
    "\n",
    "1. From the training results, it seems like the Dense layer is doing the bulk of the detection. Hence it will be good to compare with a model that only has Dense layers.\n",
    "2. Build a GAN to generate and detect palindrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_train(list_of_histories):\n",
    "    \n",
    "    if 'acc' in list_of_histories[0].history:\n",
    "        meas='acc'\n",
    "        loc='lower right'\n",
    "    else:\n",
    "        meas='loss'\n",
    "        loc='upper right'\n",
    "    \n",
    "    train_meas = []\n",
    "    val_meas = []\n",
    "    for hist in list_of_histories:\n",
    "        train_meas = train_meas + hist.history[meas]\n",
    "        val_meas = val_meas + hist.history['val_'+meas]\n",
    "\n",
    "    plt.plot(train_meas)\n",
    "    plt.plot(val_meas)\n",
    "    plt.title('model '+meas)\n",
    "    plt.ylabel(meas)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS_LIST = list(string.ascii_lowercase);\n",
    "print(CHARS_LIST);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectors_to_letters(vectors):\n",
    "    letters = [CHARS_LIST[np.argmax(x)] for x in vectors]\n",
    "    return \"\".join(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_CHARS = len(CHARS_LIST);\n",
    "PALINDROME_SIZE = 10;\n",
    "\n",
    "def generate_palindrome_or_not():\n",
    "    while True:\n",
    "        # 1 to make palindrome\n",
    "        # 0 to make sequence of random characters\n",
    "        make_palindrome = random.randint(0, 1)\n",
    "\n",
    "        letters = [];\n",
    "        for i in range( math.ceil( PALINDROME_SIZE/(make_palindrome+1) )):\n",
    "            letter_vector = np.zeros(NUM_OF_CHARS, dtype=np.int_)\n",
    "            rand_idx = random.randint(0, NUM_OF_CHARS-1);\n",
    "            letter_vector[rand_idx] = 1\n",
    "            letters.append(letter_vector.tolist())\n",
    "\n",
    "        palindrome = letters;\n",
    "        for i in range(PALINDROME_SIZE - len(letters) -1, -1, -1):\n",
    "            palindrome.append(letters[i])\n",
    "\n",
    "        yield (palindrome, make_palindrome)\n",
    "\n",
    "        \n",
    "x, y = next(generate_palindrome_or_not())\n",
    "print(\"One sample - Input shape is:\", np.array(x).shape)\n",
    "print(\"One sample - Output shape is:\", np.array(y).shape)\n",
    "print(\"One sample - Input text is:\\n\", vectors_to_letters(x))\n",
    "print(\"One sample - Input vector is:\\n\", np.array(x))\n",
    "print(\"One sample - Output is:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64;\n",
    "EPOCH_SIZE = 2000;\n",
    "\n",
    "def batch_for_network_generator():\n",
    "    while True:\n",
    "        batch_of_sentences = [ next(generate_palindrome_or_not()) for i in range(BATCH_SIZE) ]\n",
    "        X, Y = map(np.array, zip(*batch_of_sentences))\n",
    "        yield X, Y\n",
    "\n",
    "        \n",
    "X, Y = next(batch_for_network_generator())\n",
    "print(\"Batched - Input shape is:\", X.shape)\n",
    "print(\"Batched - Output shape is:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_with_model(model, batch_generator):\n",
    "    \n",
    "    test, actual = next(batch_generator())\n",
    "    \n",
    "    for i in range(20):\n",
    "        a, b = next(batch_generator())\n",
    "        test = np.concatenate((test, a))\n",
    "        actual = np.concatenate((actual, b))\n",
    "    \n",
    "    preds = model.predict(test)\n",
    "\n",
    "    test = [vectors_to_letters(x) for x in test]\n",
    "\n",
    "\n",
    "    for pred in preds:\n",
    "        pred[pred>=0.5] = 1\n",
    "        pred[pred<0.5] = 0\n",
    "\n",
    "    preds = preds.astype(\"uint8\").reshape(preds.shape[0]);\n",
    "\n",
    "    comparison = [test, actual, preds]\n",
    "    comparison = np.array(comparison).T.tolist()\n",
    "    wrongs = [elem for elem in comparison if elem[1] != elem[2]]\n",
    "    false_positives = [elem for elem in wrongs if elem[2] == \"1\"]\n",
    "    false_negatives = [elem for elem in wrongs if elem[2] == \"0\"]\n",
    "    \n",
    "    \n",
    "    print(\"Predicted correctly:\", ((len(comparison)-len(wrongs))*100.0/len(comparison)), \"%\")\n",
    "    \n",
    "    if(len(wrongs) != 0):\n",
    "        print(\"Out of the wrongs, percentage of false positives:\", (len(false_positives)*100.0/len(wrongs)), \"%\")\n",
    "        print(\"Out of the wrongs, percentage of false negatives:\", (len(false_negatives)*100.0/len(wrongs)), \"%\")\n",
    "    print(\"Wrongs\")\n",
    "    print([\"Test\", \"Actual\", \"Predicted\"])\n",
    "    print(np.array(wrongs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     filepath=\"checkpoints/Palindrome-LSTM_bidirectional-weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='loss', save_best_only=True, mode='min') # , verbose=1\n",
    "reduce_LR = ReduceLROnPlateau(monitor='loss',factor = 0.9, patience=3,cooldown=2, min_lr = 0.00001)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=50) #, min_delta=0.0001)\n",
    "callbacks_list = [reduce_LR, early_stopping] # , checkpoint ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Unidirectional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(EXECUTION_MODE == \"train\"):\n",
    "    \n",
    "    inp = Input(shape=(PALINDROME_SIZE, NUM_OF_CHARS))\n",
    "    print('our input shape is ',(PALINDROME_SIZE, NUM_OF_CHARS) )\n",
    "    x = LSTM(8)(inp)\n",
    "#     x = Dropout(0.2)(x) # Dropout is commented to remove randomness for better comparison\n",
    "    output = Dense(1, activation ='sigmoid')(x)\n",
    "    \n",
    "    \n",
    "    adam = Adam(lr=0.01)\n",
    "    unidirectional_model = Model(inputs = inp, outputs=output )\n",
    "    unidirectional_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    unidirectional_model.summary()\n",
    "\n",
    "    \n",
    "    list_of_histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(EXECUTION_MODE == \"train\"):\n",
    "    \n",
    "    history = unidirectional_model.fit_generator(\n",
    "        batch_for_network_generator(),\n",
    "        steps_per_epoch=BATCH_SIZE,\n",
    "        validation_data=batch_for_network_generator(),\n",
    "        validation_steps=BATCH_SIZE/4,\n",
    "        epochs=EPOCH_SIZE,\n",
    "        callbacks=callbacks_list\n",
    "    )\n",
    "\n",
    "    list_of_histories.append(history)\n",
    "\n",
    "    plot_train(list_of_histories)\n",
    "    \n",
    "# BATCH_SIZE = 64\n",
    "# PALINDROME_SIZE = 10\n",
    "# NUM_OF_CHARS = 26\n",
    "# PATIENCE = 50\n",
    "\n",
    "# 1 x LSTM(256) - val_acc at 0.9990 after 94 epochs, peak val_acc at 0.9990 after 43 epochs\n",
    "\n",
    "# 1 x LSTM(32) - val_acc at 0.9922 after 121 epochs, peak val_acc at 0.9971 after 70 epochs\n",
    "# 1 x LSTM(32) - val_acc at 0.9941 after 201 epochs, peak val_acc at 0.9990 after 150 epochs\n",
    "# 1 x LSTM(32) - val_acc at 0.9941 after 284 epochs, peak val_acc at 1.0000 after 233 epochs\n",
    "# 1 x LSTM(32) - val_acc at 0.9932 after 384 epochs, peak val_acc at 1.0000 after 333 epochs\n",
    "# 1 x LSTM(32) - val_acc at 0.9980 after 439 epochs, peak val_acc at 1.0000 after 388 epochs\n",
    "# 1 x LSTM(32) - val_acc at 0.9980 after 000 epochs, peak val_acc at 0.9990 after 000 epochs\n",
    "\n",
    "# 1 x LSTM(16) - val_acc at 0.9951 after 242 epochs, peak val_acc at 0.9990 after 191 epochs\n",
    "# 1 x LSTM(16) - val_acc at 0.9961 after 334 epochs, peak val_acc at 1.0000 after 283 epochs\n",
    "\n",
    "# 1 x LSTM(16) - val_acc at 0.9912 after 190 epochs, peak val_acc at 0.9922 after 139 epochs\n",
    "# 1 x LSTM(16) - val_acc at 0.9912 after 262 epochs, peak val_acc at 0.9990 after 211 epochs\n",
    "# 1 x LSTM(16) - val_acc at 0.9873 after 333 epochs, peak val_acc at 0.9961 after 282 epochs\n",
    "# 1 x LSTM(16) - val_acc at 0.9873 after 483 epochs, peak val_acc at 0.9980 after 432 epochs\n",
    "# 1 x LSTM(16) - val_acc at 0.9883 after 564 epochs, peak val_acc at 0.9961 after 211 epochs\n",
    "# 1 x LSTM(16) - val_acc at 0.9883 after 622 epochs, peak val_acc at 0.9951 after 571 epochs\n",
    "\n",
    "# 1 x LSTM(8) - val_acc at 0.9824 after 259 epochs, peak val_acc at 0.9863 after 208 epochs\n",
    "# 1 x LSTM(8) - val_acc at 0.9746 after 321 epochs, peak val_acc at 0.9854 after 270 epochs\n",
    "# 1 x LSTM(8) - val_acc at 0.9795 after 417 epochs, peak val_acc at 0.9863 after 366 epochs\n",
    "# 1 x LSTM(8) - val_acc at 0.9775 after 496 epochs, peak val_acc at 0.9873 after 445 epochs\n",
    "# 1 x LSTM(8) - val_acc at 0.9805 after 584 epochs, peak val_acc at 0.9883 after 533 epochs\n",
    "# 1 x LSTM(8) - val_acc at 0.9785 after 662 epochs, peak val_acc at 0.9854 after 611 epochs\n",
    "# 1 x LSTM(8) - val_acc at 0.9707 after 796 epochs, peak val_acc at 0.9893 after 745 epochs\n",
    "# 1 x LSTM(8) - val_acc at 0.9795 after 926 epochs, peak val_acc at 0.9873 after 875 epochs\n",
    "# 1 x LSTM(8) - val_acc at 0.9785 after 1018 epochs, peak val_acc at 0.9873 after 967 epochs\n",
    "\n",
    "# 1 x LSTM(4) - val_acc at 0.9229 after 264 epochs, peak val_acc at 0.9443 after 213 epochs\n",
    "# 1 x LSTM(4) - val_acc at 0.9258 after 347 epochs, peak val_acc at 0.9385 after 296 epochs\n",
    "# 1 x LSTM(4) - val_acc at 0.9229 after 614 epochs, peak val_acc at 0.9414 after 563 epochs\n",
    "# 1 x LSTM(4) - val_acc at 0.9326 after 678 epochs, peak val_acc at 0.9404 after 627 epochs\n",
    "# 1 x LSTM(4) - val_acc at 0.9170 after 748 epochs, peak val_acc at 0.9443 after 697 epochs\n",
    "# 1 x LSTM(4) - val_acc at 0.9326 after 840 epochs, peak val_acc at 0.9453 after 789 epochs\n",
    "# 1 x LSTM(4) - val_acc at 0.9219 after 1042 epochs, peak val_acc at 0.9453 after 961 epochs\n",
    "# 1 x LSTM(4) - val_acc at 0.9346 after 1136 epochs, peak val_acc at 0.9463 after 1085 epochs\n",
    "\n",
    "# 1 x LSTM(2) - val_acc at 0.8281 after 208 epochs, peak val_acc at 0.8652 after 157 epochs\n",
    "# 1 x LSTM(2) - val_acc at 0.8623 after 287 epochs, peak val_acc at 0.8701 after 236 epochs\n",
    "# 1 x LSTM(2) - val_acc at 0.8389 after 367 epochs, peak val_acc at 0.8623 after 316 epochs\n",
    "# 1 x LSTM(2) - val_acc at 0.8408 after 421 epochs, peak val_acc at 0.8730 after 370 epochs\n",
    "# 1 x LSTM(2) - val_acc at 0.8252 after 482 epochs, peak val_acc at 0.8594 after 431 epochs\n",
    "# 1 x LSTM(2) - val_acc at 0.8555 after 544 epochs, peak val_acc at 0.8643 after 493 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filepath = \"Palindrome-LSTM_unidirectional_%s.h5\" % FILE_SUFFIX\n",
    "if(EXECUTION_MODE == \"train\"):\n",
    "    unidirectional_model.save(filepath)\n",
    "elif(EXECUTION_MODE == \"load_pretrained\"):\n",
    "    unidirectional_model = load_model(filepath)\n",
    "    print(\"Model is loaded from a pretrained model\")\n",
    "    unidirectional_model.summary()\n",
    "\n",
    "    \n",
    "predict_with_model(unidirectional_model, batch_for_network_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Bidirectional-Concat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(EXECUTION_MODE == \"train\"):\n",
    "    \n",
    "    inp = Input(shape=(PALINDROME_SIZE, NUM_OF_CHARS))\n",
    "    print('our input shape is ',(PALINDROME_SIZE, NUM_OF_CHARS) )\n",
    "    x = Bidirectional( LSTM(4) , input_shape=(PALINDROME_SIZE, 1),  merge_mode='concat' )(inp)\n",
    "#     x = Dropout(0.2)(x)  # Dropout is commented to remove randomness for better comparison\n",
    "    output = Dense(1, activation ='sigmoid')(x)\n",
    "    \n",
    "    adam = Adam(lr=0.01)\n",
    "    bidirectional_concat_model = Model(inputs = inp, outputs=output )\n",
    "    bidirectional_concat_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    bidirectional_concat_model.summary()\n",
    "    \n",
    "    list_of_histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(EXECUTION_MODE == \"train\"):\n",
    "\n",
    "    history = bidirectional_concat_model.fit_generator(\n",
    "        batch_for_network_generator(),\n",
    "        steps_per_epoch=BATCH_SIZE,\n",
    "        validation_data=batch_for_network_generator(),\n",
    "        validation_steps=BATCH_SIZE/4,\n",
    "        epochs=EPOCH_SIZE,\n",
    "        callbacks=callbacks_list\n",
    "    )\n",
    "\n",
    "    list_of_histories.append(history)\n",
    "\n",
    "    plot_train(list_of_histories)\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# PALINDROME_SIZE = 10\n",
    "# NUM_OF_CHARS = 26\n",
    "# PATIENCE = 50\n",
    "\n",
    "# 1 x Bidirectional(LSTM(16), \"concat\") - val_acc at 0.9961 after 116 epochs, peak val_acc at 0.9990 after 65 epochs\n",
    "# 1 x Bidirectional(LSTM(16), \"concat\") - val_acc at 0.9980 after 184 epochs, peak val_acc at 1.0000 after 133 epochs\n",
    "\n",
    "# 1 x Bidirectional(LSTM(8), \"concat\") - val_acc at 0.9775 after 131 epochs, peak val_acc at 0.9912 after 80 epochs\n",
    "# 1 x Bidirectional(LSTM(8), \"concat\") - val_acc at 0.9746 after 186 epochs, peak val_acc at 0.9873 after 135 epochs\n",
    "# 1 x Bidirectional(LSTM(8), \"concat\") - val_acc at 0.9805 after 308 epochs, peak val_acc at 0.9922 after 257 epochs\n",
    "# 1 x Bidirectional(LSTM(8), \"concat\") - val_acc at 0.9785 after 360 epochs, peak val_acc at 0.9893 after 309 epochs\n",
    "# 1 x Bidirectional(LSTM(8), \"concat\") - val_acc at 0.9824 after 424 epochs, peak val_acc at 0.9893 after 373 epochs\n",
    "\n",
    "# 1 x Bidirectional(LSTM(4), \"concat\") - val_acc at 0.9932 after 131 epochs, peak val_acc at 0.9932 after 80 epochs\n",
    "# 1 x Bidirectional(LSTM(4), \"concat\") - val_acc at 0.9863 after 203 epochs, peak val_acc at 0.9971 after 152 epochs\n",
    "# 1 x Bidirectional(LSTM(4), \"concat\") - val_acc at 0.9932 after 271 epochs, peak val_acc at 0.9971 after 220 epochs\n",
    "# 1 x Bidirectional(LSTM(4), \"concat\") - val_acc at 0.9922 after 351 epochs, peak val_acc at 0.9971 after 300 epochs\n",
    "# 1 x Bidirectional(LSTM(4), \"concat\") - val_acc at 0.9873 after 408 epochs, peak val_acc at 0.9990 after 357 epochs\n",
    "# 1 x Bidirectional(LSTM(4), \"concat\") - val_acc at 0.9951 after 472 epochs, peak val_acc at 0.9961 after 421 epochs\n",
    "# 1 x Bidirectional(LSTM(4), \"concat\") - val_acc at 0.9893 after 560 epochs, peak val_acc at 0.9971 after 509 epochs\n",
    "\n",
    "# 1 x Bidirectional(LSTM(2), \"concat\") - val_acc at 0.9316 after 253 epochs, peak val_acc at 0.9492 after 202 epochs\n",
    "# 1 x Bidirectional(LSTM(2), \"concat\") - val_acc at 0.9258 after 330 epochs, peak val_acc at 0.9492 after 279 epochs\n",
    "# 1 x Bidirectional(LSTM(2), \"concat\") - val_acc at 0.9277 after 413 epochs, peak val_acc at 0.9512 after 362 epochs\n",
    "# 1 x Bidirectional(LSTM(2), \"concat\") - val_acc at 0.9316 after 480 epochs, peak val_acc at 0.9463 after 429 epochs\n",
    "# 1 x Bidirectional(LSTM(2), \"concat\") - val_acc at 0.9326 after 542 epochs, peak val_acc at 0.9463 after 491 epochs\n",
    "\n",
    "# This round of training seems to be an anomaly\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.7197 after 159 epochs, peak val_acc at 0.7871 after 108 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.7627 after 243 epochs, peak val_acc at 0.7920 after 192 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.7480 after 310 epochs, peak val_acc at 0.7920 after 259 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.7607 after 379 epochs, peak val_acc at 0.7910 after 328 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.7549 after 443 epochs, peak val_acc at 0.7832 after 392 epochs\n",
    "\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.8633 after 76 epochs, peak val_acc at 0.9062 after 25 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.8779 after 137 epochs, peak val_acc at 0.9072 after 86 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.8926 after 201 epochs, peak val_acc at 0.9072 after 150 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.8799 after 263 epochs, peak val_acc at 0.9043 after 212 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.8818 after 400 epochs, peak val_acc at 0.9053 after 349 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.8818 after 460 epochs, peak val_acc at 0.9150 after 409 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.8857 after 551 epochs, peak val_acc at 0.9160 after 500 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.8848 after 610 epochs, peak val_acc at 0.9121 after 559 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"concat\") - val_acc at 0.8906 after 686 epochs, peak val_acc at 0.9092 after 635 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath = \"Palindrome-LSTM_bidirectional_concat_%s.h5\" % FILE_SUFFIX\n",
    "if(EXECUTION_MODE == \"train\"):\n",
    "    bidirectional_concat_model.save(filepath)\n",
    "elif(EXECUTION_MODE == \"load_pretrained\"):\n",
    "    bidirectional_concat_model = load_model(filepath)\n",
    "    print(\"Model is loaded from a pretrained model\")\n",
    "    bidirectional_concat_model.summary()\n",
    "    \n",
    "predict_with_model(bidirectional_concat_model, batch_for_network_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSTM Bidirectional-Average Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(EXECUTION_MODE == \"train\"):\n",
    "    \n",
    "    inp = Input(shape=(PALINDROME_SIZE, NUM_OF_CHARS))\n",
    "    print('our input shape is ',(PALINDROME_SIZE, NUM_OF_CHARS) )\n",
    "    x = Bidirectional( LSTM(4) , input_shape=(PALINDROME_SIZE, 1),  merge_mode='ave' )(inp)\n",
    "#     x = Dropout(0.2)(x)  # Dropout is commented to remove randomness for better comparison\n",
    "    output = Dense(1, activation ='sigmoid')(x)\n",
    "    \n",
    "    adam = Adam(lr=0.01)\n",
    "    bidirectional_ave_model = Model(inputs = inp, outputs=output )\n",
    "    bidirectional_ave_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    bidirectional_ave_model.summary()\n",
    "    \n",
    "    list_of_histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(EXECUTION_MODE == \"train\"):\n",
    "\n",
    "    history = bidirectional_ave_model.fit_generator(\n",
    "        batch_for_network_generator(),\n",
    "        steps_per_epoch=BATCH_SIZE,\n",
    "        validation_data=batch_for_network_generator(),\n",
    "        validation_steps=BATCH_SIZE/4,\n",
    "        epochs=EPOCH_SIZE,\n",
    "        callbacks=callbacks_list\n",
    "    )\n",
    "\n",
    "    list_of_histories.append(history)\n",
    "\n",
    "    plot_train(list_of_histories)\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# PALINDROME_SIZE = 10\n",
    "# NUM_OF_CHARS = 26\n",
    "# PATIENCE = 50\n",
    "\n",
    "# 1 x Bidirectional(LSTM(16), \"ave\") - val_acc at 0.9961 after 169 epochs, peak val_acc at 0.9990 after 118 epochs\n",
    "# 1 x Bidirectional(LSTM(16), \"ave\") - val_acc at 0.9980 after 234 epochs, peak val_acc at 1.0000 after 183 epochs\n",
    "# 1 x Bidirectional(LSTM(16), \"ave\") - val_acc at 0.9971 after 327 epochs, peak val_acc at 1.0000 after 276 epochs\n",
    "# 1 x Bidirectional(LSTM(16), \"ave\") - val_acc at 0.9971 after 391 epochs, peak val_acc at 1.0000 after 340 epochs\n",
    "\n",
    "# 1 x Bidirectional(LSTM(8), \"ave\") - val_acc at 0.9971 after 173 epochs, peak val_acc at 0.9971 after 122 epochs\n",
    "# 1 x Bidirectional(LSTM(8), \"ave\") - val_acc at 0.9922 after 291 epochs, peak val_acc at 0.9990 after 240 epochs\n",
    "# 1 x Bidirectional(LSTM(8), \"ave\") - val_acc at 0.9961 after 389 epochs, peak val_acc at 1.0000 after 338 epochs\n",
    "# 1 x Bidirectional(LSTM(8), \"ave\") - val_acc at 0.9961 after 450 epochs, peak val_acc at 0.9980 after 399 epochs\n",
    "# 1 x Bidirectional(LSTM(8), \"ave\") - val_acc at 0.9932 after 504 epochs, peak val_acc at 0.9990 after 403 epochs\n",
    "# 1 x Bidirectional(LSTM(8), \"ave\") - val_acc at 0.9932 after 607 epochs, peak val_acc at 1.0000 after 556 epochs\n",
    "# 1 x Bidirectional(LSTM(8), \"ave\") - val_acc at 0.9922 after 715 epochs, peak val_acc at 0.9990 after 664 epochs\n",
    "# 1 x Bidirectional(LSTM(8), \"ave\") - val_acc at 0.9941 after 770 epochs, peak val_acc at 0.9990 after 719 epochs\n",
    "\n",
    "# 1 x Bidirectional(LSTM(4), \"ave\") - val_acc at 0.9648 after 129 epochs, peak val_acc at 0.9736 after 78 epochs\n",
    "# 1 x Bidirectional(LSTM(4), \"ave\") - val_acc at 0.9609 after 301 epochs, peak val_acc at 0.9795 after 250 epochs\n",
    "# 1 x Bidirectional(LSTM(4), \"ave\") - val_acc at 0.9668 after 413 epochs, peak val_acc at 0.9775 after 362 epochs\n",
    "# 1 x Bidirectional(LSTM(4), \"ave\") - val_acc at 0.9648 after 503 epochs, peak val_acc at 0.9795 after 452 epochs\n",
    "# 1 x Bidirectional(LSTM(4), \"ave\") - val_acc at 0.9727 after 609 epochs, peak val_acc at 0.9785 after 558 epochs\n",
    "# 1 x Bidirectional(LSTM(4), \"ave\") - val_acc at 0.9629 after 695 epochs, peak val_acc at 0.9785 after 644 epochs\n",
    "\n",
    "# 1 x Bidirectional(LSTM(2), \"ave\") - val_acc at 0.8301 after 138 epochs, peak val_acc at 0.9082 after 87 epochs\n",
    "# 1 x Bidirectional(LSTM(2), \"ave\") - val_acc at 0.8564 after 194 epochs, peak val_acc at 0.8652 after 143 epochs\n",
    "# 1 x Bidirectional(LSTM(2), \"ave\") - val_acc at 0.8516 after 316 epochs, peak val_acc at 0.8672 after 265 epochs\n",
    "# 1 x Bidirectional(LSTM(2), \"ave\") - val_acc at 0.8369 after 365 epochs, peak val_acc at 0.8643 after 314 epochs\n",
    "# 1 x Bidirectional(LSTM(2), \"ave\") - val_acc at 0.8281 after 430 epochs, peak val_acc at 0.8740 after 379 epochs\n",
    "# 1 x Bidirectional(LSTM(2), \"ave\") - val_acc at 0.8369 after 608 epochs, peak val_acc at 0.8809 after 557 epochs\n",
    "# 1 x Bidirectional(LSTM(2), \"ave\") - val_acc at 0.8574 after 662 epochs, peak val_acc at 0.8643 after 611 epochs\n",
    "# 1 x Bidirectional(LSTM(2), \"ave\") - val_acc at 0.8359 after 757 epochs, peak val_acc at 0.8701 after 706 epochs\n",
    "# 1 x Bidirectional(LSTM(2), \"ave\") - val_acc at 0.8350 after 811 epochs, peak val_acc at 0.8760 after 760 epochs\n",
    "\n",
    "# 1 x Bidirectional(LSTM(1), \"ave\") - val_acc at 0.8623 after 102 epochs, peak val_acc at 0.8936 after 51 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"ave\") - val_acc at 0.8721 after 197 epochs, peak val_acc at 0.8984 after 146 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"ave\") - val_acc at 0.8857 after 252 epochs, peak val_acc at 0.8857 after 201 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"ave\") - val_acc at 0.8594 after 384 epochs, peak val_acc at 0.8896 after 333 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"ave\") - val_acc at 0.8662 after 447 epochs, peak val_acc at 0.8896 after 396 epochs\n",
    "# 1 x Bidirectional(LSTM(1), \"ave\") - val_acc at 0.8633 after 535 epochs, peak val_acc at 0.8896 after 484 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Palindrome-LSTM_bidirectional_ave_%s.h5\" % FILE_SUFFIX\n",
    "if(EXECUTION_MODE == \"train\"):\n",
    "    bidirectional_ave_model.save(filepath)\n",
    "elif(EXECUTION_MODE == \"load_pretrained\"):\n",
    "    bidirectional_ave_model = load_model(filepath)\n",
    "    print(\"Model is loaded from a pretrained model\")\n",
    "    bidirectional_ave_model.summary()\n",
    "    \n",
    "predict_with_model(bidirectional_ave_model, batch_for_network_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invert Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_for_network_inverter():\n",
    "    while True:\n",
    "        X, Y = next(batch_for_network_generator())\n",
    "        Y ^= 1\n",
    "        yield X, Y\n",
    "        \n",
    "# next(batch_for_network_inverter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Bidirectional-Concat Model - Inverted, with \"0\" as palindrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(EXECUTION_MODE == \"train\"):\n",
    "    \n",
    "    inp = Input(shape=(PALINDROME_SIZE, NUM_OF_CHARS))\n",
    "    print('our input shape is ',(PALINDROME_SIZE, NUM_OF_CHARS) )\n",
    "    x = Bidirectional( LSTM(4) , input_shape=(PALINDROME_SIZE, 1),  merge_mode='concat' )(inp)\n",
    "#     x = Dropout(0.2)(x)  # Dropout is commented to remove randomness for better comparison\n",
    "    output = Dense(1, activation ='sigmoid')(x)\n",
    "    \n",
    "    adam = Adam(lr=0.01)\n",
    "    bidirectional_concat_inverted_model = Model(inputs = inp, outputs=output )\n",
    "    bidirectional_concat_inverted_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    bidirectional_concat_inverted_model.summary()\n",
    "    \n",
    "    list_of_histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(EXECUTION_MODE == \"train\"):\n",
    "\n",
    "    history = bidirectional_concat_inverted_model.fit_generator(\n",
    "        batch_for_network_inverter(),\n",
    "        steps_per_epoch=BATCH_SIZE,\n",
    "        validation_data=batch_for_network_inverter(),\n",
    "        validation_steps=BATCH_SIZE/4,\n",
    "        epochs=EPOCH_SIZE,\n",
    "        callbacks=callbacks_list\n",
    "    )\n",
    "\n",
    "    list_of_histories.append(history)\n",
    "\n",
    "    plot_train(list_of_histories)\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# PALINDROME_SIZE = 10\n",
    "# NUM_OF_CHARS = 26\n",
    "# PATIENCE = 50\n",
    "\n",
    "# 1 x Bi(LSTM(16), \"concat\"), inverted - val_acc at 0.9990 after 170 epochs, peak val_acc at 1.0000 after 119 epochs\n",
    "# 1 x Bi(LSTM(16), \"concat\"), inverted - val_acc at 0.9980 after 225 epochs, peak val_acc at 1.0000 after 174 epochs\n",
    "# 1 x Bi(LSTM(16), \"concat\"), inverted - val_acc at 0.9990 after 278 epochs, peak val_acc at 1.0000 after 227 epochs\n",
    "# 1 x Bi(LSTM(16), \"concat\"), inverted - val_acc at 1.0000 after 330 epochs, peak val_acc at 1.0000 after 279 epochs\n",
    "# 1 x Bi(LSTM(16), \"concat\"), inverted - val_acc at 0.9990 after 386 epochs, peak val_acc at 1.0000 after 335 epochs\n",
    "# 1 x Bi(LSTM(16), \"concat\"), inverted - val_acc at 1.0000 after 438 epochs, peak val_acc at 1.0000 after 387 epochs\n",
    "\n",
    "# 1 x Bi(LSTM(8), \"concat\"), inverted - val_acc at 0.9941 after 265 epochs, peak val_acc at 0.9961 after 214 epochs\n",
    "# 1 x Bi(LSTM(8), \"concat\"), inverted - val_acc at 0.9902 after 332 epochs, peak val_acc at 0.9951 after 281 epochs\n",
    "# 1 x Bi(LSTM(8), \"concat\"), inverted - val_acc at 0.9912 after 393 epochs, peak val_acc at 0.9941 after 342 epochs\n",
    "# 1 x Bi(LSTM(8), \"concat\"), inverted - val_acc at 0.9902 after 488 epochs, peak val_acc at 0.9951 after 437 epochs\n",
    "# 1 x Bi(LSTM(8), \"concat\"), inverted - val_acc at 0.9844 after 546 epochs, peak val_acc at 0.9951 after 495 epochs\n",
    "# 1 x Bi(LSTM(8), \"concat\"), inverted - val_acc at 0.9883 after 611 epochs, peak val_acc at 0.9951 after 560 epochs\n",
    "\n",
    "# 1 x Bi(LSTM(4), \"concat\"), inverted - val_acc at 0.9629 after 152 epochs, peak val_acc at 0.9678 after 101 epochs\n",
    "# 1 x Bi(LSTM(4), \"concat\"), inverted - val_acc at 0.9639 after 224 epochs, peak val_acc at 0.9717 after 173 epochs\n",
    "# 1 x Bi(LSTM(4), \"concat\"), inverted - val_acc at 0.9609 after 325 epochs, peak val_acc at 0.9746 after 274 epochs\n",
    "# 1 x Bi(LSTM(4), \"concat\"), inverted - val_acc at 0.9658 after 427 epochs, peak val_acc at 0.9795 after 376 epochs\n",
    "# 1 x Bi(LSTM(4), \"concat\"), inverted - val_acc at 0.9619 after 526 epochs, peak val_acc at 0.9795 after 475 epochs\n",
    "# 1 x Bi(LSTM(4), \"concat\"), inverted - val_acc at 0.9551 after 633 epochs, peak val_acc at 0.9756 after 582 epochs\n",
    "# 1 x Bi(LSTM(4), \"concat\"), inverted - val_acc at 0.9678 after 717 epochs, peak val_acc at 0.9648 after 666 epochs\n",
    "# 1 x Bi(LSTM(4), \"concat\"), inverted - val_acc at 0.9531 after 835 epochs, peak val_acc at 0.9775 after 784 epochs\n",
    "# 1 x Bi(LSTM(4), \"concat\"), inverted - val_acc at 0.9668 after 890 epochs, peak val_acc at 0.9756 after 839 epochs\n",
    "# 1 x Bi(LSTM(4), \"concat\"), inverted - val_acc at 0.9609 after 966 epochs, peak val_acc at 0.9775 after 915 epochs\n",
    "\n",
    "# 1 x Bi(LSTM(2), \"concat\"), inverted - val_acc at 0.9512 after 166 epochs, peak val_acc at 0.9619 after 115 epochs\n",
    "# 1 x Bi(LSTM(2), \"concat\"), inverted - val_acc at 0.9463 after 233 epochs, peak val_acc at 0.9639 after 182 epochs\n",
    "# 1 x Bi(LSTM(2), \"concat\"), inverted - val_acc at 0.9551 after 345 epochs, peak val_acc at 0.9697 after 294 epochs\n",
    "# 1 x Bi(LSTM(2), \"concat\"), inverted - val_acc at 0.9580 after 453 epochs, peak val_acc at 0.9697 after 402 epochs\n",
    "# 1 x Bi(LSTM(2), \"concat\"), inverted - val_acc at 0.9443 after 547 epochs, peak val_acc at 0.9678 after 496 epochs\n",
    "# 1 x Bi(LSTM(2), \"concat\"), inverted - val_acc at 0.9590 after 641 epochs, peak val_acc at 0.9707 after 590 epochs\n",
    "# 1 x Bi(LSTM(2), \"concat\"), inverted - val_acc at 0.9541 after 716 epochs, peak val_acc at 0.9717 after 665 epochs\n",
    "# 1 x Bi(LSTM(2), \"concat\"), inverted - val_acc at 0.9561 after 783 epochs, peak val_acc at 0.9697 after 732 epochs\n",
    "# 1 x Bi(LSTM(2), \"concat\"), inverted - val_acc at 0.9512 after 851 epochs, peak val_acc at 0.9707 after 800 epochs\n",
    "\n",
    "# 1 x Bi(LSTM(1), \"concat\"), inverted - val_acc at 0.8760 after 74 epochs, peak val_acc at 0.9033 after 23 epochs\n",
    "# 1 x Bi(LSTM(1), \"concat\"), inverted - val_acc at 0.8711 after 131 epochs, peak val_acc at 0.9023 after 80 epochs\n",
    "# 1 x Bi(LSTM(1), \"concat\"), inverted - val_acc at 0.8770 after 299 epochs, peak val_acc at 0.9102 after 248 epochs\n",
    "# 1 x Bi(LSTM(1), \"concat\"), inverted - val_acc at 0.8867 after 374 epochs, peak val_acc at 0.9062 after 323 epochs\n",
    "# 1 x Bi(LSTM(1), \"concat\"), inverted - val_acc at 0.8711 after 552 epochs, peak val_acc at 0.9062 after 501 epochs\n",
    "# 1 x Bi(LSTM(1), \"concat\"), inverted - val_acc at 0.8770 after 612 epochs, peak val_acc at 0.9072 after 561 epochs\n",
    "# 1 x Bi(LSTM(1), \"concat\"), inverted - val_acc at 0.8965 after 701 epochs, peak val_acc at 0.8955 after 650 epochs\n",
    "# 1 x Bi(LSTM(1), \"concat\"), inverted - val_acc at 0.8740 after 779 epochs, peak val_acc at 0.9209 after 728 epochs\n",
    "# 1 x Bi(LSTM(1), \"concat\"), inverted - val_acc at 0.8594 after 839 epochs, peak val_acc at 0.9053 after 788 epochs\n",
    "# 1 x Bi(LSTM(1), \"concat\"), inverted - val_acc at 0.8643 after 891 epochs, peak val_acc at 0.9014 after 840 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Palindrome-LSTM_bidirectional_concat_inverted_%s.h5\" % FILE_SUFFIX\n",
    "if(EXECUTION_MODE == \"train\"):\n",
    "    bidirectional_concat_inverted_model.save(filepath)\n",
    "elif(EXECUTION_MODE == \"load_pretrained\"):\n",
    "    bidirectional_concat_inverted_model = load_model(filepath)\n",
    "    print(\"Model is loaded from a pretrained model\")\n",
    "    bidirectional_concat_inverted_model.summary()\n",
    "\n",
    "    \n",
    "predict_with_model(bidirectional_concat_inverted_model, batch_for_network_inverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
